{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cfc2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Using previously grouped data...\n",
      "Training: 1114 trees\n",
      "Test: 268 trees\n",
      "============================================================\n",
      "COMPREHENSIVE MODEL SEARCH\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Testing: Enhanced_LBP\n",
      "Features: ['enhanced_lbp']\n",
      "Aggregation: ['mean']\n",
      "============================================================\n",
      "Re-extracting training features...\n",
      "Extracting features with types: ['enhanced_lbp']\n",
      "Aggregation methods: ['mean']\n",
      "Feature dimension per image: 54\n",
      "Total feature dimension per tree: 54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [00:51<00:00,  7.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (1114, 54)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(64), np.str_('Beech'): np.int64(264), np.str_('Douglas Fir'): np.int64(294), np.str_('Oak'): np.int64(36), np.str_('Pine'): np.int64(40), np.str_('Red Oak'): np.int64(162), np.str_('Spruce'): np.int64(254)}\n",
      "\n",
      "Re-extracting test features...\n",
      "Extracting features with types: ['enhanced_lbp']\n",
      "Aggregation methods: ['mean']\n",
      "Feature dimension per image: 54\n",
      "Total feature dimension per tree: 54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [00:12<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (268, 54)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(14), np.str_('Beech'): np.int64(64), np.str_('Douglas Fir'): np.int64(72), np.str_('Oak'): np.int64(8), np.str_('Pine'): np.int64(10), np.str_('Red Oak'): np.int64(38), np.str_('Spruce'): np.int64(62)}\n",
      "\n",
      "Applying SMOTE...\n",
      "After SMOTE: (1114, 54) -> (2058, 54)\n",
      "Scaling features...\n",
      "Running grid search...\n",
      "Grid completed in 5.6s\n",
      "Best balanced accuracy (CV): 0.9339\n",
      "Best parameters: {'C': 100, 'class_weight': None, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Test accuracy: 0.7388\n",
      "Test balanced accuracy: 0.7244\n",
      "\n",
      "============================================================\n",
      "Testing: LBP+GLCM\n",
      "Features: ['enhanced_lbp', 'glcm']\n",
      "Aggregation: ['mean']\n",
      "============================================================\n",
      "Re-extracting training features...\n",
      "Extracting features with types: ['enhanced_lbp', 'glcm']\n",
      "Aggregation methods: ['mean']\n",
      "Feature dimension per image: 94\n",
      "Total feature dimension per tree: 94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [01:02<00:00,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (1114, 94)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(64), np.str_('Beech'): np.int64(264), np.str_('Douglas Fir'): np.int64(294), np.str_('Oak'): np.int64(36), np.str_('Pine'): np.int64(40), np.str_('Red Oak'): np.int64(162), np.str_('Spruce'): np.int64(254)}\n",
      "\n",
      "Re-extracting test features...\n",
      "Extracting features with types: ['enhanced_lbp', 'glcm']\n",
      "Aggregation methods: ['mean']\n",
      "Feature dimension per image: 94\n",
      "Total feature dimension per tree: 94\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [00:14<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (268, 94)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(14), np.str_('Beech'): np.int64(64), np.str_('Douglas Fir'): np.int64(72), np.str_('Oak'): np.int64(8), np.str_('Pine'): np.int64(10), np.str_('Red Oak'): np.int64(38), np.str_('Spruce'): np.int64(62)}\n",
      "\n",
      "Applying SMOTE...\n",
      "After SMOTE: (1114, 94) -> (2058, 94)\n",
      "Scaling features...\n",
      "Running grid search...\n",
      "Grid completed in 5.9s\n",
      "Best balanced accuracy (CV): 0.9441\n",
      "Best parameters: {'C': 30, 'class_weight': None, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Test accuracy: 0.7724\n",
      "Test balanced accuracy: 0.6741\n",
      "\n",
      "============================================================\n",
      "Testing: All_Features\n",
      "Features: ['enhanced_lbp', 'glcm', 'statistical']\n",
      "Aggregation: ['mean']\n",
      "============================================================\n",
      "Re-extracting training features...\n",
      "Extracting features with types: ['enhanced_lbp', 'glcm', 'statistical']\n",
      "Aggregation methods: ['mean']\n",
      "Feature dimension per image: 100\n",
      "Total feature dimension per tree: 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [01:07<00:00,  9.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (1114, 100)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(64), np.str_('Beech'): np.int64(264), np.str_('Douglas Fir'): np.int64(294), np.str_('Oak'): np.int64(36), np.str_('Pine'): np.int64(40), np.str_('Red Oak'): np.int64(162), np.str_('Spruce'): np.int64(254)}\n",
      "\n",
      "Re-extracting test features...\n",
      "Extracting features with types: ['enhanced_lbp', 'glcm', 'statistical']\n",
      "Aggregation methods: ['mean']\n",
      "Feature dimension per image: 100\n",
      "Total feature dimension per tree: 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [00:16<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (268, 100)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(14), np.str_('Beech'): np.int64(64), np.str_('Douglas Fir'): np.int64(72), np.str_('Oak'): np.int64(8), np.str_('Pine'): np.int64(10), np.str_('Red Oak'): np.int64(38), np.str_('Spruce'): np.int64(62)}\n",
      "\n",
      "Applying SMOTE...\n",
      "After SMOTE: (1114, 100) -> (2058, 100)\n",
      "Scaling features...\n",
      "Running grid search...\n",
      "Grid completed in 6.0s\n",
      "Best balanced accuracy (CV): 0.9441\n",
      "Best parameters: {'C': 30, 'class_weight': None, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Test accuracy: 0.7799\n",
      "Test balanced accuracy: 0.6999\n",
      "\n",
      "============================================================\n",
      "Testing: Multi_Aggregation\n",
      "Features: ['enhanced_lbp']\n",
      "Aggregation: ['mean', 'std']\n",
      "============================================================\n",
      "Re-extracting training features...\n",
      "Extracting features with types: ['enhanced_lbp']\n",
      "Aggregation methods: ['mean', 'std']\n",
      "Feature dimension per image: 54\n",
      "Total feature dimension per tree: 108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [00:50<00:00,  7.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (1114, 108)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(64), np.str_('Beech'): np.int64(264), np.str_('Douglas Fir'): np.int64(294), np.str_('Oak'): np.int64(36), np.str_('Pine'): np.int64(40), np.str_('Red Oak'): np.int64(162), np.str_('Spruce'): np.int64(254)}\n",
      "\n",
      "Re-extracting test features...\n",
      "Extracting features with types: ['enhanced_lbp']\n",
      "Aggregation methods: ['mean', 'std']\n",
      "Feature dimension per image: 54\n",
      "Total feature dimension per tree: 108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [00:12<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete!\n",
      "Feature matrix shape: (268, 108)\n",
      "Species: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "Label distribution: {np.str_('Ash'): np.int64(14), np.str_('Beech'): np.int64(64), np.str_('Douglas Fir'): np.int64(72), np.str_('Oak'): np.int64(8), np.str_('Pine'): np.int64(10), np.str_('Red Oak'): np.int64(38), np.str_('Spruce'): np.int64(62)}\n",
      "\n",
      "Applying SMOTE...\n",
      "After SMOTE: (1114, 108) -> (2058, 108)\n",
      "Scaling features...\n",
      "Running grid search...\n",
      "Grid completed in 6.6s\n",
      "Best balanced accuracy (CV): 0.9164\n",
      "Best parameters: {'C': 100, 'class_weight': None, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Test accuracy: 0.7127\n",
      "Test balanced accuracy: 0.6221\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "============================================================\n",
      "    Configuration  Feature_Dim CV_Balanced_Acc Test_Accuracy Test_Balanced_Acc  Best_C Best_Gamma Class_Weight\n",
      "     Enhanced_LBP           54          0.9339        0.7388            0.7244     100       auto         None\n",
      "         LBP+GLCM           94          0.9441        0.7724            0.6741      30      scale         None\n",
      "     All_Features          100          0.9441        0.7799            0.6999      30      scale         None\n",
      "Multi_Aggregation          108          0.9164        0.7127            0.6221     100      scale         None\n",
      "\n",
      "============================================================\n",
      "BEST CONFIGURATION DETAILS\n",
      "============================================================\n",
      "Configuration: LBP+GLCM\n",
      "Feature types: ['enhanced_lbp', 'glcm']\n",
      "Aggregation: ['mean']\n",
      "Feature dimension: 94\n",
      "Best parameters: {'C': 30, 'class_weight': None, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "CV Balanced Accuracy: 0.9441\n",
      "Test Accuracy: 0.7724\n",
      "Test Balanced Accuracy: 0.6741\n",
      "\n",
      "CLASSIFICATION REPORT (Best Configuration):\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ash       0.44      0.50      0.47        14\n",
      "       Beech       0.83      0.92      0.87        64\n",
      " Douglas Fir       0.74      0.76      0.75        72\n",
      "         Oak       0.50      0.38      0.43         8\n",
      "        Pine       0.75      0.60      0.67        10\n",
      "     Red Oak       0.91      0.82      0.86        38\n",
      "      Spruce       0.78      0.74      0.76        62\n",
      "\n",
      "    accuracy                           0.77       268\n",
      "   macro avg       0.71      0.67      0.69       268\n",
      "weighted avg       0.77      0.77      0.77       268\n",
      "\n",
      "\n",
      "CONFUSION MATRIX (Best Configuration):\n",
      "--------------------------------------------------\n",
      "             Ash  Beech  Douglas Fir  Oak  Pine  Red Oak  Spruce\n",
      "Ash            7      4            0    2     0        0       1\n",
      "Beech          3     59            1    1     0        0       0\n",
      "Douglas Fir    2      2           55    0     0        1      12\n",
      "Oak            1      1            1    3     0        2       0\n",
      "Pine           0      2            2    0     6        0       0\n",
      "Red Oak        3      2            2    0     0       31       0\n",
      "Spruce         0      1           13    0     2        0      46\n",
      "\n",
      "============================================================\n",
      "COMPARISON WITH YOUR 3D RESULTS\n",
      "============================================================\n",
      "Your 3D FPFH + SMOTE + RBF SVM: 84% accuracy\n",
      "Best 2D approach: 77.2% accuracy\n",
      "Improvement strategies applied:\n",
      "✓ SMOTE for class balancing\n",
      "✓ Balanced accuracy optimization\n",
      "✓ Class weight parameter\n",
      "✓ Enhanced feature extraction\n",
      "✓ Comprehensive grid search\n",
      "✓ Multi-scale LBP features\n",
      "✓ Additional texture features (GLCM)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Improved 2D Tree Species Classification Pipeline\n",
    "# Multiple strategies to boost accuracy from 70% closer to 84%\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE  # pip install imbalanced-learn\n",
    "\n",
    "# Computer Vision imports\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage import exposure, filters\n",
    "from skimage.measure import shannon_entropy\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 1: Enhanced LBP Feature Extraction (Multiple Scales & Parameters)\n",
    "# =============================================================================\n",
    "\n",
    "def extract_enhanced_lbp_features(image_path, lbp_configs=None):\n",
    "    \"\"\"\n",
    "    Extract LBP features with multiple configurations for richer representation.\n",
    "    \"\"\"\n",
    "    if lbp_configs is None:\n",
    "        # Multiple LBP configurations for multi-scale analysis\n",
    "        lbp_configs = [\n",
    "            {'P': 8, 'R': 1, 'method': 'uniform'},    # Fine details\n",
    "            {'P': 16, 'R': 2, 'method': 'uniform'},   # Medium details  \n",
    "            {'P': 24, 'R': 3, 'method': 'uniform'},   # Coarse details\n",
    "        ]\n",
    "    \n",
    "    try:\n",
    "        # Load image in grayscale\n",
    "        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None\n",
    "            \n",
    "        # Apply slight Gaussian blur to reduce noise\n",
    "        image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "        \n",
    "        all_features = []\n",
    "        \n",
    "        for config in lbp_configs:\n",
    "            P, R, method = config['P'], config['R'], config['method']\n",
    "            \n",
    "            # Compute LBP\n",
    "            lbp = local_binary_pattern(image, P, R, method=method)\n",
    "            \n",
    "            # Calculate histogram\n",
    "            bins = P + 2 if method == 'uniform' else 2**P\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins))\n",
    "            \n",
    "            # Normalize histogram\n",
    "            hist = hist.astype(float)\n",
    "            if hist.sum() > 0:\n",
    "                hist /= hist.sum()\n",
    "                \n",
    "            all_features.append(hist)\n",
    "        \n",
    "        # Concatenate all LBP features\n",
    "        combined_features = np.concatenate(all_features)\n",
    "        return combined_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 2: Alternative Feature Extraction Methods\n",
    "# =============================================================================\n",
    "\n",
    "def extract_glcm_features(image_path, distances=[1, 2], angles=[0, 45, 90, 135]):\n",
    "    \"\"\"\n",
    "    Extract Gray-Level Co-occurrence Matrix (GLCM) features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None\n",
    "            \n",
    "        # Reduce gray levels for GLCM computation (speeds up and reduces noise)\n",
    "        image = (image // 32).astype(np.uint8)  # 8 gray levels\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for distance in distances:\n",
    "            for angle in angles:\n",
    "                # Compute GLCM\n",
    "                glcm = graycomatrix(image, [distance], [np.radians(angle)], \n",
    "                                 levels=8, symmetric=True, normed=True)\n",
    "                \n",
    "                # Extract texture properties\n",
    "                contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "                dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
    "                homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
    "                energy = graycoprops(glcm, 'energy')[0, 0]\n",
    "                correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
    "                \n",
    "                features.extend([contrast, dissimilarity, homogeneity, energy, correlation])\n",
    "        \n",
    "        return np.array(features)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing GLCM for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_statistical_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract basic statistical features from the image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None\n",
    "            \n",
    "        features = []\n",
    "        \n",
    "        # Basic statistics\n",
    "        features.append(np.mean(image))\n",
    "        features.append(np.std(image))\n",
    "        features.append(np.var(image))\n",
    "        features.append(shannon_entropy(image))\n",
    "        \n",
    "        # Gradient magnitude statistics\n",
    "        grad_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        grad_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n",
    "        \n",
    "        features.append(np.mean(grad_mag))\n",
    "        features.append(np.std(grad_mag))\n",
    "        \n",
    "        return np.array(features)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing statistical features for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_combined_features(image_path, feature_types=['enhanced_lbp', 'glcm', 'statistical']):\n",
    "    \"\"\"\n",
    "    Extract and combine multiple types of features.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    if 'enhanced_lbp' in feature_types:\n",
    "        lbp_feat = extract_enhanced_lbp_features(image_path)\n",
    "        if lbp_feat is not None:\n",
    "            all_features.append(lbp_feat)\n",
    "    \n",
    "    if 'glcm' in feature_types:\n",
    "        glcm_feat = extract_glcm_features(image_path)\n",
    "        if glcm_feat is not None:\n",
    "            all_features.append(glcm_feat)\n",
    "    \n",
    "    if 'statistical' in feature_types:\n",
    "        stat_feat = extract_statistical_features(image_path)\n",
    "        if stat_feat is not None:\n",
    "            all_features.append(stat_feat)\n",
    "    \n",
    "    if all_features:\n",
    "        return np.concatenate(all_features)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 3: Improved Multi-view Aggregation\n",
    "# =============================================================================\n",
    "\n",
    "def extract_multiview_features_improved(grouped_data, feature_types=['enhanced_lbp'], \n",
    "                                       aggregation_methods=['mean']):\n",
    "    \"\"\"\n",
    "    Extract features with multiple aggregation strategies.\n",
    "    \"\"\"\n",
    "    X_features = []\n",
    "    y_labels = []\n",
    "    \n",
    "    print(f\"Extracting features with types: {feature_types}\")\n",
    "    print(f\"Aggregation methods: {aggregation_methods}\")\n",
    "    \n",
    "    # Get feature dimension by testing on first image\n",
    "    first_species = list(grouped_data.keys())[0]\n",
    "    first_tree = list(grouped_data[first_species].keys())[0]\n",
    "    first_image = grouped_data[first_species][first_tree][0]\n",
    "    test_features = extract_combined_features(first_image, feature_types)\n",
    "    \n",
    "    if test_features is None:\n",
    "        raise ValueError(\"Could not extract features from test image\")\n",
    "    \n",
    "    feature_dim = len(test_features)\n",
    "    print(f\"Feature dimension per image: {feature_dim}\")\n",
    "    \n",
    "    # Calculate total feature dimension based on aggregation methods\n",
    "    total_dim = feature_dim * len(aggregation_methods)\n",
    "    print(f\"Total feature dimension per tree: {total_dim}\\n\")\n",
    "    \n",
    "    for species_name, trees in tqdm(grouped_data.items(), desc=\"Processing species\"):\n",
    "        for tree_name, image_paths in tqdm(trees.items(), desc=f\"{species_name} trees\", leave=False):\n",
    "            \n",
    "            # Extract features from all views of this tree\n",
    "            tree_features = []\n",
    "            \n",
    "            for img_path in image_paths:\n",
    "                features = extract_combined_features(img_path, feature_types)\n",
    "                if features is not None:\n",
    "                    tree_features.append(features)\n",
    "            \n",
    "            # Aggregate features using multiple methods\n",
    "            if tree_features:\n",
    "                tree_features_array = np.stack(tree_features)\n",
    "                aggregated_features = []\n",
    "                \n",
    "                for agg_method in aggregation_methods:\n",
    "                    if agg_method == 'mean':\n",
    "                        agg_feat = np.mean(tree_features_array, axis=0)\n",
    "                    elif agg_method == 'std':\n",
    "                        agg_feat = np.std(tree_features_array, axis=0)\n",
    "                    elif agg_method == 'max':\n",
    "                        agg_feat = np.max(tree_features_array, axis=0)\n",
    "                    elif agg_method == 'min':\n",
    "                        agg_feat = np.min(tree_features_array, axis=0)\n",
    "                    else:\n",
    "                        agg_feat = np.mean(tree_features_array, axis=0)  # default\n",
    "                    \n",
    "                    aggregated_features.append(agg_feat)\n",
    "                \n",
    "                final_features = np.concatenate(aggregated_features)\n",
    "                X_features.append(final_features)\n",
    "                y_labels.append(species_name)\n",
    "    \n",
    "    X_features = np.array(X_features)\n",
    "    y_labels = np.array(y_labels)\n",
    "    \n",
    "    # Create label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "    \n",
    "    print(f\"Feature extraction complete!\")\n",
    "    print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "    print(f\"Species: {list(label_encoder.classes_)}\")\n",
    "    print(f\"Label distribution: {dict(zip(*np.unique(y_labels, return_counts=True)))}\\n\")\n",
    "    \n",
    "    return X_features, y_encoded, label_encoder\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY 4: Comprehensive Grid Search with SMOTE and Class Balancing\n",
    "# =============================================================================\n",
    "\n",
    "def comprehensive_model_search(X_train, y_train, X_test, y_test, label_encoder):\n",
    "    \"\"\"\n",
    "    Comprehensive model search similar to your 3D approach.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"COMPREHENSIVE MODEL SEARCH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Feature configurations to test\n",
    "    feature_configs = [\n",
    "        {\n",
    "            'name': 'Enhanced_LBP',\n",
    "            'types': ['enhanced_lbp'],\n",
    "            'aggregation': ['mean']\n",
    "        },\n",
    "        {\n",
    "            'name': 'LBP+GLCM',\n",
    "            'types': ['enhanced_lbp', 'glcm'],\n",
    "            'aggregation': ['mean']\n",
    "        },\n",
    "        {\n",
    "            'name': 'All_Features',\n",
    "            'types': ['enhanced_lbp', 'glcm', 'statistical'],\n",
    "            'aggregation': ['mean']\n",
    "        },\n",
    "        {\n",
    "            'name': 'Multi_Aggregation',\n",
    "            'types': ['enhanced_lbp'],\n",
    "            'aggregation': ['mean', 'std']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    best_summary = None\n",
    "    summaries = []\n",
    "    \n",
    "    for config in feature_configs:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {config['name']}\")\n",
    "        print(f\"Features: {config['types']}\")\n",
    "        print(f\"Aggregation: {config['aggregation']}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Re-extract features with this configuration\n",
    "            print(\"Re-extracting training features...\")\n",
    "            X_train_config, y_train_config, _ = extract_multiview_features_improved(\n",
    "                train_grouped, config['types'], config['aggregation'])\n",
    "            \n",
    "            print(\"Re-extracting test features...\")\n",
    "            X_test_config, y_test_config, _ = extract_multiview_features_improved(\n",
    "                test_grouped, config['types'], config['aggregation'])\n",
    "            \n",
    "            # SMOTE to balance classes (following your 3D approach)\n",
    "            print(\"Applying SMOTE...\")\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_bal, y_train_bal = smote.fit_resample(X_train_config, y_train_config)\n",
    "            print(f\"After SMOTE: {X_train_config.shape} -> {X_train_bal.shape}\")\n",
    "            \n",
    "            # Scale features\n",
    "            print(\"Scaling features...\")\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_bal)\n",
    "            X_test_scaled = scaler.transform(X_test_config)\n",
    "            \n",
    "            # Comprehensive parameter grid (following your 3D approach)\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 3, 10, 30, 100],\n",
    "                'gamma': ['scale', 'auto', 0.03, 0.1, 0.3, 1],\n",
    "                'class_weight': [None, 'balanced'],\n",
    "                'kernel': ['rbf']\n",
    "            }\n",
    "            \n",
    "            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            grid = GridSearchCV(\n",
    "                SVC(random_state=42), \n",
    "                param_grid, \n",
    "                cv=cv, \n",
    "                n_jobs=-1, \n",
    "                verbose=0, \n",
    "                scoring='balanced_accuracy'  # Use balanced_accuracy like your 3D approach\n",
    "            )\n",
    "            \n",
    "            print(\"Running grid search...\")\n",
    "            start = time.time()\n",
    "            grid.fit(X_train_scaled, y_train_bal)\n",
    "            dur = time.time() - start\n",
    "            \n",
    "            print(f\"Grid completed in {dur:.1f}s\")\n",
    "            print(f\"Best balanced accuracy (CV): {grid.best_score_:.4f}\")\n",
    "            print(f\"Best parameters: {grid.best_params_}\")\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            best_svm = grid.best_estimator_\n",
    "            y_pred = best_svm.predict(X_test_scaled)\n",
    "            \n",
    "            test_acc = accuracy_score(y_test_config, y_pred)\n",
    "            test_bal_acc = balanced_accuracy_score(y_test_config, y_pred)\n",
    "            \n",
    "            print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "            print(f\"Test balanced accuracy: {test_bal_acc:.4f}\")\n",
    "            \n",
    "            # Store results\n",
    "            summary = {\n",
    "                'config_name': config['name'],\n",
    "                'feature_types': config['types'],\n",
    "                'aggregation': config['aggregation'],\n",
    "                'feature_dim': X_train_config.shape[1],\n",
    "                'best_params': grid.best_params_,\n",
    "                'cv_balanced_accuracy': grid.best_score_,\n",
    "                'test_accuracy': test_acc,\n",
    "                'test_balanced_accuracy': test_bal_acc,\n",
    "                'y_pred': y_pred,\n",
    "                'y_true': y_test_config\n",
    "            }\n",
    "            summaries.append(summary)\n",
    "            \n",
    "            # Track best configuration\n",
    "            if best_summary is None or summary['cv_balanced_accuracy'] > best_summary['cv_balanced_accuracy']:\n",
    "                best_summary = summary\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with configuration {config['name']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return best_summary, summaries\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA (using previous grouping functions)\n",
    "# =============================================================================\n",
    "\n",
    "# Re-use your existing grouped data\n",
    "print(\"Using previously grouped data...\")\n",
    "print(f\"Training: {len(sum([list(trees.keys()) for trees in train_grouped.values()], []))} trees\")\n",
    "print(f\"Test: {len(sum([list(trees.keys()) for trees in test_grouped.values()], []))} trees\")\n",
    "\n",
    "# =============================================================================\n",
    "# RUN COMPREHENSIVE SEARCH\n",
    "# =============================================================================\n",
    "\n",
    "best_config, all_summaries = comprehensive_model_search(\n",
    "    None, None, None, None, None  # We'll extract features inside the function\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame([{\n",
    "    'Configuration': s['config_name'],\n",
    "    'Feature_Dim': s['feature_dim'],\n",
    "    'CV_Balanced_Acc': f\"{s['cv_balanced_accuracy']:.4f}\",\n",
    "    'Test_Accuracy': f\"{s['test_accuracy']:.4f}\",\n",
    "    'Test_Balanced_Acc': f\"{s['test_balanced_accuracy']:.4f}\",\n",
    "    'Best_C': s['best_params']['C'],\n",
    "    'Best_Gamma': s['best_params']['gamma'],\n",
    "    'Class_Weight': s['best_params']['class_weight']\n",
    "} for s in all_summaries])\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BEST CONFIGURATION DETAILS\")\n",
    "print(\"=\"*60)\n",
    "if best_config:\n",
    "    print(f\"Configuration: {best_config['config_name']}\")\n",
    "    print(f\"Feature types: {best_config['feature_types']}\")\n",
    "    print(f\"Aggregation: {best_config['aggregation']}\")\n",
    "    print(f\"Feature dimension: {best_config['feature_dim']}\")\n",
    "    print(f\"Best parameters: {best_config['best_params']}\")\n",
    "    print(f\"CV Balanced Accuracy: {best_config['cv_balanced_accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {best_config['test_accuracy']:.4f}\")\n",
    "    print(f\"Test Balanced Accuracy: {best_config['test_balanced_accuracy']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nCLASSIFICATION REPORT (Best Configuration):\")\n",
    "    print(\"-\" * 50)\n",
    "    target_names = [name.replace('np.str_(\\'', '').replace('\\')', '') for name in \n",
    "                   ['Ash', 'Beech', 'Douglas Fir', 'Oak', 'Pine', 'Red Oak', 'Spruce']]\n",
    "    print(classification_report(best_config['y_true'], best_config['y_pred'], \n",
    "                              target_names=target_names))\n",
    "    \n",
    "    print(f\"\\nCONFUSION MATRIX (Best Configuration):\")\n",
    "    print(\"-\" * 50)\n",
    "    cm = confusion_matrix(best_config['y_true'], best_config['y_pred'])\n",
    "    cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "    print(cm_df)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON WITH YOUR 3D RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Your 3D FPFH + SMOTE + RBF SVM: 84% accuracy\")\n",
    "print(f\"Best 2D approach: {best_config['test_accuracy']:.1%} accuracy\" if best_config else \"No valid results\")\n",
    "print(f\"Improvement strategies applied:\")\n",
    "print(\"✓ SMOTE for class balancing\")\n",
    "print(\"✓ Balanced accuracy optimization\")\n",
    "print(\"✓ Class weight parameter\")\n",
    "print(\"✓ Enhanced feature extraction\")\n",
    "print(\"✓ Comprehensive grid search\")\n",
    "print(\"✓ Multi-scale LBP features\")\n",
    "print(\"✓ Additional texture features (GLCM)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
