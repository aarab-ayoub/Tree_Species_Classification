{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc177a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultra-enhanced libraries imported!\n",
      "================================================================================\n",
      "ULTRA-ENHANCED TREE CLASSIFICATION PIPELINE\n",
      "================================================================================\n",
      "Grouping images...\n",
      "\n",
      "============================================================\n",
      "ULTRA FEATURE EXTRACTION\n",
      "============================================================\n",
      "Extracting ultra-comprehensive features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing species: 100%|██████████| 7/7 [07:42<00:00, 66.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed extractions: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1114,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 526\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mULTRA FEATURE EXTRACTION\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    524\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m X_train, y_train, label_encoder = \u001b[43mextract_ultra_multiview_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_grouped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;66;03m# Extract test features\u001b[39;00m\n\u001b[32m    529\u001b[39m X_test_features = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 316\u001b[39m, in \u001b[36mextract_ultra_multiview_features\u001b[39m\u001b[34m(grouped_data)\u001b[39m\n\u001b[32m    312\u001b[39m                 y_labels.append(species_name)\n\u001b[32m    314\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed extractions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailed_extractions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m X_features = np.array(X_features, dtype=np.float32)\n\u001b[32m    317\u001b[39m y_labels = np.array(y_labels)\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Handle any NaN or inf values\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1114,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Ultra-Enhanced Tree Species Classification\n",
    "# Advanced techniques for maximum accuracy\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Advanced ML imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Advanced CV imports\n",
    "from skimage.feature import (local_binary_pattern, graycomatrix, graycoprops, \n",
    "                           hog, daisy, corner_harris, corner_peaks)\n",
    "from skimage import exposure, filters, morphology, segmentation\n",
    "from skimage.measure import shannon_entropy, regionprops, label\n",
    "from skimage.util import img_as_ubyte\n",
    "\n",
    "# Data augmentation and sampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Ultra-enhanced libraries imported!\")\n",
    "\n",
    "# =============================================================================\n",
    "# ULTRA ENHANCEMENT 1: Comprehensive Feature Extraction\n",
    "# =============================================================================\n",
    "\n",
    "def extract_comprehensive_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract comprehensive feature set combining multiple descriptors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None\n",
    "        \n",
    "        # Multiple preprocessing strategies\n",
    "        images = {\n",
    "            'original': image,\n",
    "            'equalized': exposure.equalize_adapthist(image, clip_limit=0.03),\n",
    "            'gamma_corrected': exposure.adjust_gamma(image, gamma=0.8),\n",
    "            'denoised': filters.gaussian(image, sigma=0.5)\n",
    "        }\n",
    "        \n",
    "        all_features = []\n",
    "        \n",
    "        for img_name, img in images.items():\n",
    "            img = img_as_ubyte(img) if img.dtype != np.uint8 else img\n",
    "            \n",
    "            # === LBP Features at Multiple Scales ===\n",
    "            lbp_scales = [(8, 1), (16, 2), (24, 3), (32, 4)]\n",
    "            for P, R in lbp_scales:\n",
    "                lbp = local_binary_pattern(img, P, R, method='uniform')\n",
    "                bins = P + 2\n",
    "                hist, _ = np.histogram(lbp.ravel(), bins=bins, range=(0, bins))\n",
    "                hist = hist.astype(float)\n",
    "                if hist.sum() > 0:\n",
    "                    hist /= hist.sum()\n",
    "                all_features.extend(hist)\n",
    "            \n",
    "            # === HOG Features ===\n",
    "            try:\n",
    "                hog_features = hog(img, \n",
    "                                 orientations=8, \n",
    "                                 pixels_per_cell=(16, 16),\n",
    "                                 cells_per_block=(2, 2), \n",
    "                                 block_norm='L2-Hys',\n",
    "                                 visualize=False)\n",
    "                all_features.extend(hog_features)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # === DAISY Features (downsampled for speed) ===\n",
    "            try:\n",
    "                small_img = cv2.resize(img, (64, 64))\n",
    "                daisy_features = daisy(small_img, \n",
    "                                     step=8, radius=15, rings=3, histograms=8,\n",
    "                                     orientations=8, visualize=False)\n",
    "                # Flatten and subsample\n",
    "                daisy_flat = daisy_features.flatten()\n",
    "                if len(daisy_flat) > 200:  # Subsample if too large\n",
    "                    indices = np.linspace(0, len(daisy_flat)-1, 200, dtype=int)\n",
    "                    daisy_flat = daisy_flat[indices]\n",
    "                all_features.extend(daisy_flat)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Use only original image for computationally expensive features\n",
    "        img = images['original']\n",
    "        img = img_as_ubyte(img) if img.dtype != np.uint8 else img\n",
    "        \n",
    "        # === Enhanced GLCM Features ===\n",
    "        small_img = cv2.resize(img, (128, 128))  # Larger for better GLCM\n",
    "        \n",
    "        # Multiple distances and angles\n",
    "        distances = [1, 2, 3]\n",
    "        angles = [0, 45, 90, 135]\n",
    "        \n",
    "        for distance in distances:\n",
    "            try:\n",
    "                glcm = graycomatrix(small_img, \n",
    "                                  distances=[distance], \n",
    "                                  angles=np.deg2rad(angles), \n",
    "                                  levels=64,  # Reduced levels for speed\n",
    "                                  symmetric=True, normed=True)\n",
    "                \n",
    "                # Multiple GLCM properties\n",
    "                props = ['contrast', 'dissimilarity', 'homogeneity', \n",
    "                        'energy', 'correlation', 'ASM']\n",
    "                \n",
    "                for prop in props:\n",
    "                    try:\n",
    "                        values = graycoprops(glcm, prop).flatten()\n",
    "                        all_features.extend(values)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # === Statistical and Morphological Features ===\n",
    "        # Basic statistics\n",
    "        stats = [\n",
    "            np.mean(img), np.std(img), np.var(img),\n",
    "            np.median(img), np.min(img), np.max(img),\n",
    "            float(pd.Series(img.flatten()).skew()),\n",
    "            float(pd.Series(img.flatten()).kurtosis()),\n",
    "            shannon_entropy(img)\n",
    "        ]\n",
    "        all_features.extend(stats)\n",
    "        \n",
    "        # Percentiles\n",
    "        percentiles = [10, 25, 75, 90]\n",
    "        for p in percentiles:\n",
    "            all_features.append(np.percentile(img, p))\n",
    "        \n",
    "        # Edge and corner features\n",
    "        edges_sobel = filters.sobel(img)\n",
    "        edges_canny = cv2.Canny(img, 50, 150)\n",
    "        \n",
    "        edge_stats = [\n",
    "            np.sum(edges_sobel > 0.1) / edges_sobel.size,  # Edge density\n",
    "            np.mean(edges_sobel),\n",
    "            np.std(edges_sobel),\n",
    "            np.sum(edges_canny > 0) / edges_canny.size  # Canny edge density\n",
    "        ]\n",
    "        all_features.extend(edge_stats)\n",
    "        \n",
    "        # Corner detection\n",
    "        try:\n",
    "            corners = corner_harris(img)\n",
    "            corner_peaks_coords = corner_peaks(corners, min_distance=5, threshold_rel=0.1)\n",
    "            corner_count = len(corner_peaks_coords) / (img.shape[0] * img.shape[1])\n",
    "            all_features.append(corner_count)\n",
    "        except:\n",
    "            all_features.append(0)\n",
    "        \n",
    "        # Morphological features\n",
    "        # Binary image operations\n",
    "        thresh = img > filters.threshold_otsu(img)\n",
    "        \n",
    "        # Connected components\n",
    "        labeled = label(thresh)\n",
    "        regions = regionprops(labeled)\n",
    "        \n",
    "        if regions:\n",
    "            # Area statistics\n",
    "            areas = [r.area for r in regions]\n",
    "            all_features.extend([\n",
    "                len(regions),  # Number of components\n",
    "                np.mean(areas) if areas else 0,\n",
    "                np.std(areas) if areas else 0,\n",
    "                np.max(areas) if areas else 0\n",
    "            ])\n",
    "            \n",
    "            # Shape features from largest component\n",
    "            largest_region = max(regions, key=lambda r: r.area)\n",
    "            all_features.extend([\n",
    "                largest_region.eccentricity,\n",
    "                largest_region.solidity,\n",
    "                largest_region.extent,\n",
    "                largest_region.major_axis_length / (largest_region.minor_axis_length + 1e-6)\n",
    "            ])\n",
    "        else:\n",
    "            all_features.extend([0] * 8)\n",
    "        \n",
    "        # Fractal dimension approximation\n",
    "        try:\n",
    "            # Box counting method approximation\n",
    "            scales = [2, 4, 8, 16]\n",
    "            counts = []\n",
    "            for scale in scales:\n",
    "                h, w = img.shape\n",
    "                boxes_h, boxes_w = h // scale, w // scale\n",
    "                if boxes_h > 0 and boxes_w > 0:\n",
    "                    resized = cv2.resize(thresh.astype(np.uint8), (boxes_w, boxes_h))\n",
    "                    count = np.sum(resized > 0)\n",
    "                    counts.append(count)\n",
    "            \n",
    "            if len(counts) > 1:\n",
    "                # Simple fractal dimension estimate\n",
    "                scales_log = np.log(scales[:len(counts)])\n",
    "                counts_log = np.log(np.array(counts) + 1)\n",
    "                fractal_dim = -np.polyfit(scales_log, counts_log, 1)[0]\n",
    "                all_features.append(fractal_dim)\n",
    "            else:\n",
    "                all_features.append(0)\n",
    "        except:\n",
    "            all_features.append(0)\n",
    "        \n",
    "        return np.array(all_features, dtype=np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# ULTRA ENHANCEMENT 2: Advanced Aggregation with View-Specific Weights\n",
    "# =============================================================================\n",
    "\n",
    "def extract_ultra_multiview_features(grouped_data):\n",
    "    \"\"\"\n",
    "    Ultra-enhanced multi-view feature extraction with view-specific processing.\n",
    "    \"\"\"\n",
    "    X_features = []\n",
    "    y_labels = []\n",
    "    \n",
    "    # Define view type importance weights\n",
    "    view_weights = {\n",
    "        'top': 1.2,      # Top view often very distinctive\n",
    "        'front': 1.0,    # Standard weight\n",
    "        'side': 1.0,     # Standard weight\n",
    "        'rot': 0.8       # Rotated views get slightly lower weight\n",
    "    }\n",
    "    \n",
    "    print(\"Extracting ultra-comprehensive features...\")\n",
    "    \n",
    "    failed_extractions = 0\n",
    "    \n",
    "    for species_name, trees in tqdm(grouped_data.items(), desc=\"Processing species\"):\n",
    "        for tree_name, image_paths in tqdm(trees.items(), desc=f\"{species_name} trees\", leave=False):\n",
    "            \n",
    "            # Group by view type\n",
    "            view_features = defaultdict(list)\n",
    "            \n",
    "            for img_path in image_paths:\n",
    "                features = extract_comprehensive_features(img_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    # Determine view type\n",
    "                    view_type = 'rot' if 'rot_' in img_path.stem else img_path.stem.split('_')[-1]\n",
    "                    view_features[view_type].append(features)\n",
    "                else:\n",
    "                    failed_extractions += 1\n",
    "            \n",
    "            if view_features:\n",
    "                # Advanced aggregation strategies\n",
    "                aggregated_features = []\n",
    "                \n",
    "                # For each view type, aggregate separately\n",
    "                for view_type, features_list in view_features.items():\n",
    "                    if features_list:\n",
    "                        features_array = np.stack(features_list)\n",
    "                        weight = view_weights.get(view_type, 1.0)\n",
    "                        \n",
    "                        # Multiple statistics with weighting\n",
    "                        view_mean = np.mean(features_array, axis=0) * weight\n",
    "                        view_max = np.max(features_array, axis=0)\n",
    "                        view_std = np.std(features_array, axis=0)\n",
    "                        \n",
    "                        aggregated_features.extend(view_mean)\n",
    "                        aggregated_features.extend(view_max * weight)\n",
    "                        aggregated_features.extend(view_std)\n",
    "                \n",
    "                # Overall statistics across all views\n",
    "                all_features = []\n",
    "                for features_list in view_features.values():\n",
    "                    all_features.extend(features_list)\n",
    "                \n",
    "                if all_features:\n",
    "                    all_features_array = np.stack(all_features)\n",
    "                    \n",
    "                    # Global statistics\n",
    "                    global_mean = np.mean(all_features_array, axis=0)\n",
    "                    global_std = np.std(all_features_array, axis=0)\n",
    "                    global_median = np.median(all_features_array, axis=0)\n",
    "                    global_range = np.max(all_features_array, axis=0) - np.min(all_features_array, axis=0)\n",
    "                    \n",
    "                    # Add global features\n",
    "                    aggregated_features.extend(global_mean)\n",
    "                    aggregated_features.extend(global_std)\n",
    "                    aggregated_features.extend(global_median)\n",
    "                    aggregated_features.extend(global_range)\n",
    "                    \n",
    "                    X_features.append(aggregated_features)\n",
    "                    y_labels.append(species_name)\n",
    "    \n",
    "    print(f\"Failed extractions: {failed_extractions}\")\n",
    "    \n",
    "    X_features = np.array(X_features, dtype=np.float32)\n",
    "    y_labels = np.array(y_labels)\n",
    "    \n",
    "    # Handle any NaN or inf values\n",
    "    X_features = np.nan_to_num(X_features, nan=0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    # Label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_labels)\n",
    "    \n",
    "    print(f\"Ultra feature extraction complete!\")\n",
    "    print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "    print(f\"Species: {list(label_encoder.classes_)}\")\n",
    "    \n",
    "    return X_features, y_encoded, label_encoder\n",
    "\n",
    "# =============================================================================\n",
    "# ULTRA ENHANCEMENT 3: Advanced Data Preprocessing and Augmentation\n",
    "# =============================================================================\n",
    "\n",
    "def advanced_preprocessing_pipeline(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing with multiple techniques.\n",
    "    \"\"\"\n",
    "    print(\"Applying advanced preprocessing...\")\n",
    "    \n",
    "    # Step 1: Remove low-variance features\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    variance_selector = VarianceThreshold(threshold=0.001)\n",
    "    X_train_var = variance_selector.fit_transform(X_train)\n",
    "    X_test_var = variance_selector.transform(X_test)\n",
    "    \n",
    "    print(f\"After variance filtering: {X_train_var.shape[1]} features\")\n",
    "    \n",
    "    # Step 2: Power transformation for better distribution\n",
    "    power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    X_train_power = power_transformer.fit_transform(X_train_var)\n",
    "    X_test_power = power_transformer.transform(X_test_var)\n",
    "    \n",
    "    # Step 3: Robust scaling (less sensitive to outliers)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_power)\n",
    "    X_test_scaled = scaler.transform(X_test_power)\n",
    "    \n",
    "    # Step 4: Feature selection\n",
    "    # Univariate feature selection\n",
    "    k_best = SelectKBest(f_classif, k=min(500, X_train_scaled.shape[1]))\n",
    "    X_train_selected = k_best.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_selected = k_best.transform(X_test_scaled)\n",
    "    \n",
    "    print(f\"After feature selection: {X_train_selected.shape[1]} features\")\n",
    "    \n",
    "    # Step 5: PCA with optimal components\n",
    "    pca = PCA(n_components=0.99, random_state=42)  # Keep 99% variance\n",
    "    X_train_pca = pca.fit_transform(X_train_selected)\n",
    "    X_test_pca = pca.transform(X_test_selected)\n",
    "    \n",
    "    print(f\"After PCA: {X_train_pca.shape[1]} components (variance: {pca.explained_variance_ratio_.sum():.4f})\")\n",
    "    \n",
    "    return X_train_pca, X_test_pca, (variance_selector, power_transformer, scaler, k_best, pca)\n",
    "\n",
    "# =============================================================================\n",
    "# ULTRA ENHANCEMENT 4: Advanced Model Ensemble with Stacking\n",
    "# =============================================================================\n",
    "\n",
    "def create_ultimate_ensemble(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Create the ultimate ensemble with multiple algorithms and stacking.\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "    \n",
    "    print(\"Creating ultimate ensemble...\")\n",
    "    \n",
    "    # Base models with different strengths\n",
    "    base_models = {\n",
    "        'svm_rbf': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42),\n",
    "        'svm_poly': SVC(kernel='poly', probability=True, class_weight='balanced', random_state=42),\n",
    "        'rf': RandomForestClassifier(n_estimators=300, class_weight='balanced', random_state=42),\n",
    "        'et': ExtraTreesClassifier(n_estimators=300, class_weight='balanced', random_state=42),\n",
    "        'gb': GradientBoostingClassifier(n_estimators=200, random_state=42),\n",
    "        'mlp': MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=1000, random_state=42),\n",
    "    }\n",
    "    \n",
    "    # Hyperparameter grids (more focused based on your results)\n",
    "    param_grids = {\n",
    "        'svm_rbf': {\n",
    "            'C': [50, 100, 200, 500],\n",
    "            'gamma': ['scale', 0.001, 0.01, 0.1]\n",
    "        },\n",
    "        'svm_poly': {\n",
    "            'C': [10, 50, 100],\n",
    "            'degree': [2, 3],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        },\n",
    "        'rf': {\n",
    "            'n_estimators': [200, 300, 500],\n",
    "            'max_depth': [20, 30, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        },\n",
    "        'et': {\n",
    "            'n_estimators': [200, 300],\n",
    "            'max_depth': [20, 30],\n",
    "            'min_samples_split': [2, 5]\n",
    "        },\n",
    "        'gb': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7]\n",
    "        },\n",
    "        'mlp': {\n",
    "            'hidden_layer_sizes': [(200, 100), (300, 150), (400, 200)],\n",
    "            'alpha': [0.001, 0.01, 0.1],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train and optimize each base model\n",
    "    optimized_models = {}\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in base_models.items():\n",
    "        print(f\"Optimizing {name}...\")\n",
    "        \n",
    "        if name in param_grids:\n",
    "            grid_search = GridSearchCV(\n",
    "                model, param_grids[name], cv=cv,\n",
    "                scoring='accuracy', n_jobs=-1, verbose=0\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            optimized_models[name] = grid_search.best_estimator_\n",
    "            print(f\"  Best {name} CV score: {grid_search.best_score_:.4f}\")\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            optimized_models[name] = model\n",
    "    \n",
    "    # Create stacked ensemble\n",
    "    estimators = [(name, model) for name, model in optimized_models.items()]\n",
    "    \n",
    "    # Meta-learner\n",
    "    meta_learner = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    stacked_ensemble = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=3,\n",
    "        stack_method='predict_proba'\n",
    "    )\n",
    "    \n",
    "    print(\"Training stacked ensemble...\")\n",
    "    stacked_ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    # Also create voting ensemble for comparison\n",
    "    voting_ensemble = VotingClassifier(\n",
    "        estimators=estimators,\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_ensemble.fit(X_train, y_train)\n",
    "    \n",
    "    return {\n",
    "        'individual_models': optimized_models,\n",
    "        'stacked_ensemble': stacked_ensemble,\n",
    "        'voting_ensemble': voting_ensemble\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN ULTRA-ENHANCED PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ULTRA-ENHANCED TREE CLASSIFICATION PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load and group data (reuse existing functions)\n",
    "def group_images_by_tree(data_dir):\n",
    "    grouped_data = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for species_dir in sorted(data_dir.iterdir()):\n",
    "        if not species_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        species_name = species_dir.name\n",
    "        \n",
    "        for img_path in species_dir.iterdir():\n",
    "            if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                base_name_parts = img_path.stem.split('_')[:-1]\n",
    "                base_tree_name = '_'.join(base_name_parts)\n",
    "                grouped_data[species_name][base_tree_name].append(img_path)\n",
    "    \n",
    "    return dict(grouped_data)\n",
    "\n",
    "# Set paths\n",
    "base_path = Path(\"../data/multi_view_images\")\n",
    "train_path = base_path / \"train\"\n",
    "test_path = base_path / \"test\"\n",
    "\n",
    "# Group data\n",
    "print(\"Grouping images...\")\n",
    "train_grouped = group_images_by_tree(train_path)\n",
    "test_grouped = group_images_by_tree(test_path)\n",
    "\n",
    "# Extract ultra-comprehensive features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ULTRA FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, y_train, label_encoder = extract_ultra_multiview_features(train_grouped)\n",
    "\n",
    "# Extract test features\n",
    "X_test_features = []\n",
    "y_test_labels = []\n",
    "\n",
    "for species_name, trees in tqdm(test_grouped.items(), desc=\"Processing test species\"):\n",
    "    for tree_name, image_paths in tqdm(trees.items(), desc=f\"{species_name} test trees\", leave=False):\n",
    "        \n",
    "        view_features = defaultdict(list)\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            features = extract_comprehensive_features(img_path)\n",
    "            if features is not None:\n",
    "                view_type = 'rot' if 'rot_' in img_path.stem else img_path.stem.split('_')[-1]\n",
    "                view_features[view_type].append(features)\n",
    "        \n",
    "        if view_features:\n",
    "            # Same aggregation as training\n",
    "            view_weights = {'top': 1.2, 'front': 1.0, 'side': 1.0, 'rot': 0.8}\n",
    "            aggregated_features = []\n",
    "            \n",
    "            for view_type, features_list in view_features.items():\n",
    "                if features_list:\n",
    "                    features_array = np.stack(features_list)\n",
    "                    weight = view_weights.get(view_type, 1.0)\n",
    "                    \n",
    "                    view_mean = np.mean(features_array, axis=0) * weight\n",
    "                    view_max = np.max(features_array, axis=0)\n",
    "                    view_std = np.std(features_array, axis=0)\n",
    "                    \n",
    "                    aggregated_features.extend(view_mean)\n",
    "                    aggregated_features.extend(view_max * weight)\n",
    "                    aggregated_features.extend(view_std)\n",
    "            \n",
    "            all_features = []\n",
    "            for features_list in view_features.values():\n",
    "                all_features.extend(features_list)\n",
    "            \n",
    "            if all_features:\n",
    "                all_features_array = np.stack(all_features)\n",
    "                \n",
    "                global_mean = np.mean(all_features_array, axis=0)\n",
    "                global_std = np.std(all_features_array, axis=0)\n",
    "                global_median = np.median(all_features_array, axis=0)\n",
    "                global_range = np.max(all_features_array, axis=0) - np.min(all_features_array, axis=0)\n",
    "                \n",
    "                aggregated_features.extend(global_mean)\n",
    "                aggregated_features.extend(global_std)\n",
    "                aggregated_features.extend(global_median)\n",
    "                aggregated_features.extend(global_range)\n",
    "                \n",
    "                X_test_features.append(aggregated_features)\n",
    "                y_test_labels.append(species_name)\n",
    "\n",
    "X_test = np.array(X_test_features, dtype=np.float32)\n",
    "X_test = np.nan_to_num(X_test, nan=0, posinf=1e6, neginf=-1e6)\n",
    "y_test = label_encoder.transform(y_test_labels)\n",
    "\n",
    "print(f\"Ultra-comprehensive features extracted!\")\n",
    "print(f\"Training shape: {X_train.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\")\n",
    "\n",
    "# Advanced preprocessing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train_processed, X_test_processed, preprocessors = advanced_preprocessing_pipeline(\n",
    "    X_train, X_test, y_train\n",
    ")\n",
    "\n",
    "# Handle class imbalance with SMOTE\n",
    "print(\"\\nApplying SMOTE for class balancing...\")\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {Counter(y_train)}\")\n",
    "print(f\"After SMOTE: {Counter(y_train_balanced)}\")\n",
    "\n",
    "# Create and train ultimate ensemble\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ULTIMATE ENSEMBLE TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_results = create_ultimate_ensemble(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_to_evaluate = {\n",
    "    'Best Individual SVM': ensemble_results['individual_models']['svm_rbf'],\n",
    "    'Best Random Forest': ensemble_results['individual_models']['rf'],\n",
    "    'Best Gradient Boosting': ensemble_results['individual_models']['gb'],\n",
    "    'Voting Ensemble': ensemble_results['voting_ensemble'],\n",
    "    'Stacked Ensemble': ensemble_results['stacked_ensemble']\n",
    "}\n",
    "\n",
    "results = {}\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model in models_to_evaluate.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    improvement = acc - 0.6978  # vs original\n",
    "    print(f\"{name:25s}: {acc:.4f} (+{improvement:.4f})\")\n",
    "\n",
    "# Detailed report for best model\n",
    "print(f\"\\nDETAILED REPORT FOR BEST MODEL ({max(results, key=results.get)}):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "y_pred_best = best_model.predict(X_test_processed)\n",
    "target_names = label_encoder.classes_\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=target_names))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "cm_df = pd.DataFrame(cm, index=target_names, columns=target_names)\n",
    "print(cm_df)\n",
    "\n",
    "# Calculate improvement\n",
    "original_accuracy = 0.6978\n",
    "best_result = max(results.values())\n",
    "total_improvement = best_result - original_accuracy\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original Accuracy:     {original_accuracy:.4f} (69.78%)\")\n",
    "print(f\"Best New Accuracy:     {best_result:.4f} ({best_result*100:.2f}%)\")\n",
    "print(f\"Total Improvement:     +{total_improvement:.4f} (+{total_improvement*100:.2f}%)\")\n",
    "print(f\"Relative Improvement:  {(total_improvement/original_accuracy)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n🎯 TARGET ACHIEVED: {best_result*100:.1f}% accuracy!\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
