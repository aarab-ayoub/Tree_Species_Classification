{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee11a02",
   "metadata": {},
   "source": [
    "# Tree Species Classification using LBP Features + RBF SVM\n",
    "# Complete Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0115601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Scikit-image imports\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182ac4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_images_by_tree(data_dir):\n",
    "    \"\"\"\n",
    "    Groups image files by their base tree name.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Path to directory containing species folders\n",
    "        \n",
    "    Returns:\n",
    "        dict: Nested dictionary {species: {tree_name: [list_of_image_paths]}}\n",
    "    \"\"\"\n",
    "    tree_groups = defaultdict(lambda: defaultdict(list))\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Warning: Directory {data_dir} does not exist!\")\n",
    "        return {}\n",
    "    \n",
    "    # Iterate through each species folder\n",
    "    for species_folder in data_path.iterdir():\n",
    "        if not species_folder.is_dir():\n",
    "            continue\n",
    "            \n",
    "        species_name = species_folder.name\n",
    "        print(f\"Processing species: {species_name}\")\n",
    "        \n",
    "        # Get all image files in the species folder\n",
    "        image_files = []\n",
    "        for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
    "            image_files.extend(species_folder.glob(ext))\n",
    "        \n",
    "        # Group images by tree name\n",
    "        for image_path in image_files:\n",
    "            # Extract tree name from filename (everything before the first underscore followed by view info)\n",
    "            filename = image_path.stem\n",
    "            \n",
    "            # Pattern to match: tree_XXX_viewinfo\n",
    "            # We want to extract \"tree_XXX\" as the base name\n",
    "            match = re.match(r'(tree_\\d+)', filename)\n",
    "            if match:\n",
    "                tree_name = match.group(1)\n",
    "                tree_groups[species_name][tree_name].append(str(image_path))\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract tree name from {filename}\")\n",
    "    \n",
    "    # Print summary\n",
    "    total_trees = 0\n",
    "    total_images = 0\n",
    "    for species, trees in tree_groups.items():\n",
    "        num_trees = len(trees)\n",
    "        num_images = sum(len(images) for images in trees.values())\n",
    "        total_trees += num_trees\n",
    "        total_images += num_images\n",
    "        print(f\"  {species}: {num_trees} trees, {num_images} images\")\n",
    "    \n",
    "    print(f\"Total: {total_trees} trees, {total_images} images\")\n",
    "    return dict(tree_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d45e683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 2: LBP Feature Extraction Function\n",
    "# =============================================================================\n",
    "\n",
    "def extract_lbp_features(image_path, P=24, R=3, method='uniform'):\n",
    "    \"\"\"\n",
    "    Extract LBP features from a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        P (int): Number of circularly symmetric neighbor points\n",
    "        R (int): Radius of circle\n",
    "        method (str): Method for LBP computation\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Normalized LBP histogram (feature vector)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image in grayscale\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"Error: Could not load image {image_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Compute LBP\n",
    "        lbp = local_binary_pattern(image, P, R, method=method)\n",
    "        \n",
    "        # Calculate histogram\n",
    "        if method == 'uniform':\n",
    "            # For uniform LBPs, we have P+2 bins (uniform patterns + non-uniform)\n",
    "            n_bins = P + 2\n",
    "        else:\n",
    "            # For other methods, we have 2^P possible patterns\n",
    "            n_bins = 2 ** P\n",
    "        \n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "        \n",
    "        # Normalize histogram\n",
    "        hist = hist.astype(float)\n",
    "        hist /= (hist.sum() + 1e-10)  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        return hist\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4137ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Function to Process All Images and Create Feature Vectors\n",
    "# =============================================================================\n",
    "\n",
    "def create_feature_vectors(tree_groups, P=24, R=3, method='uniform'):\n",
    "    \"\"\"\n",
    "    Create feature vectors by combining LBP features from multiple views of each tree.\n",
    "    \n",
    "    Args:\n",
    "        tree_groups (dict): Dictionary from group_images_by_tree function\n",
    "        P, R, method: LBP parameters\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y, tree_names) where X is feature matrix, y is labels, tree_names is list of identifiers\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    tree_names = []\n",
    "    \n",
    "    print(\"Extracting features from images...\")\n",
    "    \n",
    "    for species_name, trees in tree_groups.items():\n",
    "        print(f\"\\nProcessing {species_name}...\")\n",
    "        \n",
    "        for tree_name, image_paths in tqdm(trees.items(), desc=f\"{species_name}\"):\n",
    "            # Extract LBP features from all views of this tree\n",
    "            tree_features = []\n",
    "            \n",
    "            for image_path in image_paths:\n",
    "                lbp_hist = extract_lbp_features(image_path, P, R, method)\n",
    "                \n",
    "                if lbp_hist is not None:\n",
    "                    tree_features.append(lbp_hist)\n",
    "                else:\n",
    "                    print(f\"Skipping {image_path} due to extraction error\")\n",
    "            \n",
    "            # If we successfully extracted features from at least one image\n",
    "            if tree_features:\n",
    "                # Average the histograms across all views\n",
    "                averaged_features = np.mean(tree_features, axis=0)\n",
    "                \n",
    "                X.append(averaged_features)\n",
    "                y.append(species_name)\n",
    "                tree_names.append(f\"{species_name}_{tree_name}\")\n",
    "            else:\n",
    "                print(f\"Warning: No features extracted for {species_name}_{tree_name}\")\n",
    "    \n",
    "    return np.array(X), np.array(y), tree_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882c7690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: GROUPING IMAGES BY TREE NAME\n",
      "============================================================\n",
      "\n",
      "Grouping training images:\n",
      "Warning: Directory data/multi_view_images/train does not exist!\n",
      "\n",
      "Grouping test images:\n",
      "Warning: Directory data/multi_view_images/test does not exist!\n",
      "\n",
      "============================================================\n",
      "STEP 2: EXTRACTING LBP FEATURES\n",
      "============================================================\n",
      "\n",
      "Extracting training features...\n",
      "Extracting features from images...\n",
      "\n",
      "Extracting test features...\n",
      "Extracting features from images...\n",
      "\n",
      "Feature extraction complete!\n",
      "Training data shape: (0,)\n",
      "Test data shape: (0,)\n",
      "Number of classes: 0\n",
      "Classes: []\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Main Processing - Load and Prepare Data\n",
    "# =============================================================================\n",
    "\n",
    "# Set your data paths\n",
    "TRAIN_DIR = \"data/multi_view_images/train\"\n",
    "TEST_DIR = \"data/multi_view_images/test\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: GROUPING IMAGES BY TREE NAME\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group training images\n",
    "print(\"\\nGrouping training images:\")\n",
    "train_groups = group_images_by_tree(TRAIN_DIR)\n",
    "\n",
    "# Group test images\n",
    "print(\"\\nGrouping test images:\")\n",
    "test_groups = group_images_by_tree(TEST_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: EXTRACTING LBP FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract features for training data\n",
    "print(\"\\nExtracting training features...\")\n",
    "X_train, y_train, train_names = create_feature_vectors(train_groups, P=24, R=3, method='uniform')\n",
    "\n",
    "# Extract features for test data\n",
    "print(\"\\nExtracting test features...\")\n",
    "X_test, y_test, test_names = create_feature_vectors(test_groups, P=24, R=3, method='uniform')\n",
    "\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "# print(f\"Feature vector length: {X_train.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca4dfef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: DATA PREPROCESSING\n",
      "============================================================\n",
      "Label mapping:\n",
      "\n",
      "Scaling features...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mScaling features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m scaler = StandardScaler()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m X_train_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m X_test_scaled = scaler.transform(X_test)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScaling complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:907\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:943\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    911\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    912\u001b[39m \n\u001b[32m    913\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    942\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/prjt/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1091\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1084\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1085\u001b[39m             msg = (\n\u001b[32m   1086\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1087\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1088\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUSV\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1095\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1096\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1097\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 5: Data Preprocessing - Scaling and Label Encoding\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: DATA PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"Label mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name}: {i}\")\n",
    "\n",
    "# Scale features\n",
    "print(f\"\\nScaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaling complete!\")\n",
    "print(f\"Training features - Mean: {X_train_scaled.mean():.4f}, Std: {X_train_scaled.std():.4f}\")\n",
    "print(f\"Test features - Mean: {X_test_scaled.mean():.4f}, Std: {X_test_scaled.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Hyperparameter Tuning with GridSearchCV\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define parameter grid for RBF SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid:\")\n",
    "print(f\"  C: {param_grid['C']}\")\n",
    "print(f\"  gamma: {param_grid['gamma']}\")\n",
    "\n",
    "# Create SVM classifier\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Create stratified k-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "print(f\"\\nPerforming grid search with 5-fold cross-validation...\")\n",
    "print(f\"This may take several minutes depending on data size...\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "print(f\"\\nGrid search complete!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 7: Train Final Model and Evaluate\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: FINAL MODEL TRAINING AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"FINAL RESULTS\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Cross-validation Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a8973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 8: Detailed Evaluation - Classification Report and Confusion Matrix\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"DETAILED EVALUATION\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=label_encoder.classes_, \n",
    "           yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Tree Species Classification')\n",
    "plt.xlabel('Predicted Species')\n",
    "plt.ylabel('True Species')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# CELL 9: Additional Analysis - Per-Class Performance\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "# Calculate per-class metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_per_class = precision_score(y_test_encoded, y_pred, average=None)\n",
    "recall_per_class = recall_score(y_test_encoded, y_pred, average=None)\n",
    "f1_per_class = f1_score(y_test_encoded, y_pred, average=None)\n",
    "\n",
    "print(f\"\\nPer-class performance:\")\n",
    "print(f\"{'Species':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    support = np.sum(y_test_encoded == i)\n",
    "    print(f\"{class_name:<12} {precision_per_class[i]:<10.4f} {recall_per_class[i]:<10.4f} \"\n",
    "          f\"{f1_per_class[i]:<10.4f} {support:<10}\")\n",
    "\n",
    "# Overall averages\n",
    "print(\"-\" * 60)\n",
    "print(f\"Macro avg    {np.mean(precision_per_class):<10.4f} {np.mean(recall_per_class):<10.4f} \"\n",
    "      f\"{np.mean(f1_per_class):<10.4f} {len(y_test_encoded):<10}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"PIPELINE EXECUTION COMPLETE!\")\n",
    "print(f\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
