{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a0bab1",
   "metadata": {},
   "source": [
    "# üéØ OPTIMIZED Data Fusion - Class Imbalance Solutions\n",
    "\n",
    "## Critical Issues Addressed:\n",
    "1. **üö® Pine Class Crisis**: Only 1 test sample - EXTREME imbalance  \n",
    "2. **üå≤ Spruce Discrimination**: Confused with Douglas Fir (similar species)\n",
    "3. **‚öñÔ∏è Class Balance**: Advanced sampling and loss functions\n",
    "4. **üß† Feature Discrimination**: Better fusion architecture\n",
    "5. **üìä Data Augmentation**: Class-specific augmentations\n",
    "6. **üé≤ Synthetic Data**: Smart oversampling for minorities\n",
    "\n",
    "**Target: Fix Pine (0% ‚Üí 70%+) and Spruce (0% ‚Üí 60%+) recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb144ffa",
   "metadata": {},
   "source": [
    "# üî¨ Enhanced Data Fusion Pipeline - Advanced Multi-Modal Architecture\n",
    "\n",
    "## Critical Issues Identified & Fixed:\n",
    "1. **Low accuracy (37%)**: Simple concatenation fusion was suboptimal\n",
    "2. **Frozen backbone**: Using frozen ResNet18 with poor feature quality\n",
    "3. **Feature scale mismatch**: CNN (512-dim) vs LBP (26-dim) imbalance\n",
    "4. **Basic fusion**: Simple concatenation without learnable interactions\n",
    "\n",
    "## Key Enhancements Applied:\n",
    "1. **üéØ Fine-tuned CNN Branch**: Replace frozen ResNet with adapted feature extractor\n",
    "2. **‚öñÔ∏è Feature Normalization**: LayerNorm to balance CNN and LBP contributions  \n",
    "3. **üö™ Gated Fusion Mechanism**: Learnable gates to control information flow\n",
    "4. **üìö Two-Stage Training**: Stage 1 (LBP branch) ‚Üí Stage 2 (end-to-end fine-tuning)\n",
    "5. **üéõÔ∏è WeightedRandomSampler**: Address class imbalance in batches\n",
    "6. **üî• Focal Loss**: Focus on hard examples from minority classes\n",
    "7. **üß† Advanced Architecture**: Enhanced fusion head with attention mechanisms\n",
    "\n",
    "Expected improvement: **37% ‚Üí 60%+ accuracy** through better feature integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2e0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ ENHANCED DATA FUSION PIPELINE\n",
      "============================================================\n",
      "   Device: mps\n",
      "   Strategy: Two-Stage Training + Gated Fusion\n",
      "   Architecture: Fine-tuned CNN + Enhanced LBP + Gated Fusion\n",
      "   Stage 1: 20 epochs @ LR 0.001 (LBP branch only)\n",
      "   Stage 2: 60 epochs @ LR 5e-05 (end-to-end)\n",
      "   Enhanced LBP: radius=2, points=16, bins=18\n",
      "   Expected improvement: 37% ‚Üí 60%+ accuracy\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED Data Fusion Pipeline - Imports and Configuration\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, RandomRotation, ColorJitter, RandomHorizontalFlip, RandomResizedCrop, TrivialAugmentWide\n",
    "from skimage.feature import local_binary_pattern\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ENHANCED Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "STAGE1_LR = 1e-3          # ENHANCEMENT: Stage 1 - LBP branch training\n",
    "STAGE2_LR = 5e-5          # ENHANCEMENT: Stage 2 - End-to-end fine-tuning  \n",
    "STAGE1_EPOCHS = 20        # ENHANCEMENT: Two-stage training\n",
    "STAGE2_EPOCHS = 60\n",
    "TOTAL_EPOCHS = STAGE1_EPOCHS + STAGE2_EPOCHS\n",
    "PATIENCE = 20\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ENHANCED LBP parameters  \n",
    "LBP_RADIUS = 2            # ENHANCEMENT: Larger radius for richer texture features\n",
    "LBP_N_POINTS = 16         # ENHANCEMENT: More points for better discrimination\n",
    "LBP_FEATURE_DIM = 18      # 16 + 2 for uniform LBP\n",
    "\n",
    "print(\"üî¨ ENHANCED DATA FUSION PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Strategy: Two-Stage Training + Gated Fusion\")\n",
    "print(f\"   Architecture: Fine-tuned CNN + Enhanced LBP + Gated Fusion\")\n",
    "print(f\"   Stage 1: {STAGE1_EPOCHS} epochs @ LR {STAGE1_LR} (LBP branch only)\")\n",
    "print(f\"   Stage 2: {STAGE2_EPOCHS} epochs @ LR {STAGE2_LR} (end-to-end)\")\n",
    "print(f\"   Enhanced LBP: radius={LBP_RADIUS}, points={LBP_N_POINTS}, bins={LBP_FEATURE_DIM}\")\n",
    "print(f\"   Expected improvement: 37% ‚Üí 60%+ accuracy\")\n",
    "\n",
    "# ENHANCEMENT: Focal Loss Implementation for Data Fusion\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance in fusion model\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a7e2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Ash' 'Beech' 'Douglas Fir' 'Oak' 'Pine' 'Red Oak' 'Spruce']\n",
      "Train: 344, Val: 86, Test: 102\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading and Splitting Functions\n",
    "def load_tree_data(data_path):\n",
    "    data_path = Path(data_path)\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"{data_path} not found!\")\n",
    "\n",
    "    for species_dir in data_path.iterdir():\n",
    "        if species_dir.is_dir():\n",
    "            species_name = species_dir.name\n",
    "            files = list(species_dir.glob(\"*.npy\"))\n",
    "            file_paths.extend(files)\n",
    "            labels.extend([species_name] * len(files))\n",
    "\n",
    "    return file_paths, labels\n",
    "\n",
    "# Define data paths (adjust this relative path if needed)\n",
    "base_data_path = Path(\"../../../data/multi_view_images\")\n",
    "train_data_path = base_data_path / \"train\"\n",
    "test_data_path = base_data_path / \"test\"\n",
    "\n",
    "# Load data\n",
    "train_file_paths, train_labels = load_tree_data(train_data_path)\n",
    "test_file_paths, test_labels = load_tree_data(test_data_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = train_labels + test_labels\n",
    "label_encoder.fit(all_labels)\n",
    "train_encoded_labels = label_encoder.transform(train_labels)\n",
    "test_encoded_labels = label_encoder.transform(test_labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split training data into train/validation\n",
    "train_paths, val_paths, train_labels_enc, val_labels_enc = train_test_split(\n",
    "    train_file_paths, train_encoded_labels, test_size=0.2, random_state=42, stratify=train_encoded_labels\n",
    ")\n",
    "\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(f\"Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_file_paths)}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85aa1301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Pre-computing Enhanced LBP features...\n",
      "üì• Loading existing enhanced LBP features from enhanced_lbp_features.pkl\n",
      "üìä Loaded enhanced LBP parameters: {'radius': 2, 'n_points': 16, 'n_bins': 18}\n",
      "‚úÖ Enhanced LBP features ready:\n",
      "   Total samples: 532\n",
      "   Feature dimension: 18\n",
      "   Feature range: [0.000, 0.487]\n"
     ]
    }
   ],
   "source": [
    "# ENHANCEMENT: Advanced LBP Feature Extraction with Better Parameters\n",
    "def extract_enhanced_lbp_features(views_array, radius=LBP_RADIUS, n_points=LBP_N_POINTS, n_bins=LBP_FEATURE_DIM):\n",
    "    \"\"\"\n",
    "    Enhanced LBP feature extraction with better parameters and preprocessing.\n",
    "    Returns multi-scale LBP features for improved texture discrimination.\n",
    "    \"\"\"\n",
    "    all_lbp_histograms = []\n",
    "    \n",
    "    for view in views_array:\n",
    "        # Ensure proper format and contrast enhancement\n",
    "        if view.dtype != np.uint8:\n",
    "            view = (view * 255).astype(np.uint8)\n",
    "        \n",
    "        # ENHANCEMENT: Apply histogram equalization for better contrast\n",
    "        from skimage import exposure\n",
    "        view = exposure.equalize_hist(view)\n",
    "        view = (view * 255).astype(np.uint8)\n",
    "        \n",
    "        # Calculate enhanced LBP\n",
    "        lbp = local_binary_pattern(view, n_points, radius, method='uniform')\n",
    "        \n",
    "        # ENHANCEMENT: Multi-scale LBP - combine different radii\n",
    "        if radius > 1:\n",
    "            lbp_small = local_binary_pattern(view, n_points//2, 1, method='uniform')\n",
    "            # Combine different scales\n",
    "            lbp_combined = lbp + lbp_small * 0.5\n",
    "        else:\n",
    "            lbp_combined = lbp\n",
    "        \n",
    "        # Calculate histogram with better binning\n",
    "        hist, _ = np.histogram(lbp_combined.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "        all_lbp_histograms.append(hist)\n",
    "    \n",
    "    # ENHANCEMENT: Weighted averaging based on view quality (center views get more weight)\n",
    "    num_views = len(all_lbp_histograms)\n",
    "    weights = np.array([0.8, 1.0, 1.0, 1.0, 1.0, 0.8])  # Center views weighted higher\n",
    "    if num_views != 6:\n",
    "        weights = np.ones(num_views)  # Fallback to equal weights\n",
    "    \n",
    "    weights = weights[:num_views] / weights[:num_views].sum()\n",
    "    \n",
    "    # Weighted average across all views\n",
    "    averaged_lbp = np.average(all_lbp_histograms, axis=0, weights=weights)\n",
    "    return averaged_lbp.astype(np.float32)\n",
    "\n",
    "def precompute_enhanced_lbp_features(file_paths, save_path=\"enhanced_lbp_features.pkl\"):\n",
    "    \"\"\"\n",
    "    Pre-compute enhanced LBP features with caching and progress tracking.\n",
    "    \"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"üì• Loading existing enhanced LBP features from {save_path}\")\n",
    "        with open(save_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    print(\"üîÑ Computing Enhanced LBP features with improved parameters...\")\n",
    "    print(f\"   Parameters: radius={LBP_RADIUS}, points={LBP_N_POINTS}, bins={LBP_FEATURE_DIM}\")\n",
    "    \n",
    "    lbp_features = {}\n",
    "    total_files = len(file_paths)\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        if i % 50 == 0:  # More frequent progress updates\n",
    "            print(f\"   Processing {i:4d}/{total_files} ({i/total_files*100:.1f}%)\")\n",
    "        \n",
    "        try:\n",
    "            # Load multi-view data\n",
    "            views_arr = np.load(file_path)\n",
    "            \n",
    "            # Extract enhanced LBP features\n",
    "            lbp_feat = extract_enhanced_lbp_features(views_arr)\n",
    "            \n",
    "            # Use file path as key for easy lookup\n",
    "            lbp_features[str(file_path)] = lbp_feat\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error processing {file_path}: {e}\")\n",
    "            # Use zero features as fallback\n",
    "            lbp_features[str(file_path)] = np.zeros(LBP_FEATURE_DIM, dtype=np.float32)\n",
    "    \n",
    "    # Save to disk with metadata\n",
    "    save_data = {\n",
    "        'features': lbp_features,\n",
    "        'parameters': {\n",
    "            'radius': LBP_RADIUS,\n",
    "            'n_points': LBP_N_POINTS,\n",
    "            'n_bins': LBP_FEATURE_DIM\n",
    "        },\n",
    "        'num_files': len(lbp_features)\n",
    "    }\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(save_data, f)\n",
    "    \n",
    "    print(f\"üíæ Enhanced LBP features saved to {save_path}\")\n",
    "    print(f\"   Features computed: {len(lbp_features)}\")\n",
    "    print(f\"   Feature dimension: {LBP_FEATURE_DIM}\")\n",
    "    \n",
    "    return lbp_features\n",
    "\n",
    "# ENHANCEMENT: Pre-compute enhanced LBP features for all data splits\n",
    "print(\"üîÑ Pre-computing Enhanced LBP features...\")\n",
    "all_file_paths = train_file_paths + test_file_paths\n",
    "\n",
    "# Load or compute enhanced LBP features\n",
    "lbp_data = precompute_enhanced_lbp_features(all_file_paths, \"enhanced_lbp_features.pkl\")\n",
    "\n",
    "# Extract just the features if we loaded the full data structure\n",
    "if isinstance(lbp_data, dict) and 'features' in lbp_data:\n",
    "    lbp_features_dict = lbp_data['features']\n",
    "    print(f\"üìä Loaded enhanced LBP parameters: {lbp_data['parameters']}\")\n",
    "else:\n",
    "    lbp_features_dict = lbp_data\n",
    "\n",
    "print(f\"‚úÖ Enhanced LBP features ready:\")\n",
    "print(f\"   Total samples: {len(lbp_features_dict)}\")\n",
    "print(f\"   Feature dimension: {len(next(iter(lbp_features_dict.values())))}\")\n",
    "print(f\"   Feature range: [{min([f.min() for f in lbp_features_dict.values()]):.3f}, \"\n",
    "      f\"{max([f.max() for f in lbp_features_dict.values()]):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5931fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion datasets created:\n",
      "  Train: 344\n",
      "  Validation: 86\n",
      "  Test: 102\n",
      "\n",
      "Sample shapes:\n",
      "  Images: torch.Size([6, 3, 224, 224])\n",
      "  LBP features: torch.Size([18])\n",
      "  Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Custom FusionDataset Class\n",
    "class FusionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns both multi-view images and pre-computed LBP features.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, labels, lbp_features_dict, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.lbp_features_dict = lbp_features_dict\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load multi-view data\n",
    "        views_arr = np.load(file_path)\n",
    "        \n",
    "        # Convert to RGB PIL images for CNN\n",
    "        images = [Image.fromarray((view * 255).astype(np.uint8), mode='L').convert('RGB') \n",
    "                 for view in views_arr]\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            images = [self.transform(img) for img in images]\n",
    "        \n",
    "        # Stack views into a single tensor: (num_views, 3, H, W)\n",
    "        image_stack = torch.stack(images, dim=0)\n",
    "        \n",
    "        # Get pre-computed LBP features\n",
    "        lbp_features = self.lbp_features_dict[str(file_path)]\n",
    "        lbp_tensor = torch.tensor(lbp_features, dtype=torch.float32)\n",
    "        \n",
    "        return image_stack, lbp_tensor, label\n",
    "\n",
    "# Image Transforms\n",
    "train_transform = Compose([\n",
    "    RandomResizedCrop(size=IMG_SIZE, scale=(0.85, 1.0)),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomRotation(degrees=10),\n",
    "    ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Create fusion datasets\n",
    "train_dataset = FusionDataset(train_paths, train_labels_enc, lbp_features_dict, train_transform)\n",
    "val_dataset = FusionDataset(val_paths, val_labels_enc, lbp_features_dict, test_transform)\n",
    "test_dataset = FusionDataset(test_file_paths, test_encoded_labels, lbp_features_dict, test_transform)\n",
    "\n",
    "print(f\"Fusion datasets created:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Validation: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")\n",
    "\n",
    "# Test dataset output\n",
    "sample_images, sample_lbp, sample_label = train_dataset[0]\n",
    "print(f\"\\nSample shapes:\")\n",
    "print(f\"  Images: {sample_images.shape}\")\n",
    "print(f\"  LBP features: {sample_lbp.shape}\")\n",
    "print(f\"  Label: {sample_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a731ec84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è Creating Enhanced Fusion Model...\n",
      "   üéØ Loading fine-tuned feature extractor (if available)...\n",
      "   ‚ö†Ô∏è Could not load enhanced extractor: No module named 'method_4_enhanced_feature_extractor_svm'\n",
      "   ‚è© Using ResNet18 with fine-tuning capability...\n",
      "üî¨ Enhanced Fusion CNN Architecture:\n",
      "   CNN features: 512\n",
      "   LBP features: 18 ‚Üí 64 (processed)\n",
      "   Fusion: Gated mechanism with cross-attention\n",
      "   Classifier: Advanced 3-layer with BatchNorm\n",
      "   Expected: Significant improvement over 37% baseline\n",
      "\n",
      "üìä Enhanced Model Statistics:\n",
      "   CNN parameters: 11,176,512\n",
      "   Fusion parameters: 678,471\n",
      "   Total parameters: 11,854,983\n",
      "   Architecture: Gated fusion with cross-attention\n",
      "üî¨ Enhanced Fusion CNN Architecture:\n",
      "   CNN features: 512\n",
      "   LBP features: 18 ‚Üí 64 (processed)\n",
      "   Fusion: Gated mechanism with cross-attention\n",
      "   Classifier: Advanced 3-layer with BatchNorm\n",
      "   Expected: Significant improvement over 37% baseline\n",
      "\n",
      "üìä Enhanced Model Statistics:\n",
      "   CNN parameters: 11,176,512\n",
      "   Fusion parameters: 678,471\n",
      "   Total parameters: 11,854,983\n",
      "   Architecture: Gated fusion with cross-attention\n"
     ]
    }
   ],
   "source": [
    "# ENHANCEMENT: Advanced Gated Fusion CNN Architecture\n",
    "class GatedFusionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced gated fusion mechanism to control information flow between modalities.\n",
    "    Uses learnable gates to balance CNN and LBP feature contributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_dim, lbp_dim, hidden_dim=256):\n",
    "        super(GatedFusionModule, self).__init__()\n",
    "        \n",
    "        # Feature projection layers\n",
    "        self.cnn_proj = nn.Linear(cnn_dim, hidden_dim)\n",
    "        self.lbp_proj = nn.Linear(lbp_dim, hidden_dim)\n",
    "        \n",
    "        # Gate networks - learn how much of each modality to use\n",
    "        self.cnn_gate = nn.Sequential(\n",
    "            nn.Linear(cnn_dim + lbp_dim, hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.lbp_gate = nn.Sequential(\n",
    "            nn.Linear(cnn_dim + lbp_dim, hidden_dim // 2), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Cross-attention mechanism\n",
    "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, cnn_features, lbp_features):\n",
    "        # Project features to common space\n",
    "        cnn_proj = self.cnn_proj(cnn_features)\n",
    "        lbp_proj = self.lbp_proj(lbp_features)\n",
    "        \n",
    "        # Concatenate for gate computation\n",
    "        combined = torch.cat([cnn_features, lbp_features], dim=1)\n",
    "        \n",
    "        # Compute gates\n",
    "        cnn_gate_weights = self.cnn_gate(combined)\n",
    "        lbp_gate_weights = self.lbp_gate(combined)\n",
    "        \n",
    "        # Apply gates\n",
    "        gated_cnn = cnn_proj * cnn_gate_weights\n",
    "        gated_lbp = lbp_proj * lbp_gate_weights\n",
    "        \n",
    "        # Cross-attention between modalities\n",
    "        features_stack = torch.stack([gated_cnn, gated_lbp], dim=1)  # (batch, 2, hidden_dim)\n",
    "        attended_features, _ = self.cross_attention(features_stack, features_stack, features_stack)\n",
    "        \n",
    "        # Aggregate attended features\n",
    "        final_features = torch.mean(attended_features, dim=1)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        final_features = self.layer_norm(final_features)\n",
    "        \n",
    "        return final_features\n",
    "\n",
    "class EnhancedFusionCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced multi-modal CNN with gated fusion and fine-tuned CNN branch.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, lbp_dim=LBP_FEATURE_DIM, use_pretrained_extractor=True):\n",
    "        super(EnhancedFusionCNN, self).__init__()\n",
    "        \n",
    "        self.use_pretrained_extractor = use_pretrained_extractor\n",
    "        \n",
    "        # ENHANCEMENT: Use fine-tuned CNN branch instead of frozen ResNet\n",
    "        if use_pretrained_extractor:\n",
    "            print(\"   üéØ Loading fine-tuned feature extractor (if available)...\")\n",
    "            try:\n",
    "                # Try to load the enhanced feature extractor from method 4\n",
    "                extractor_path = \"../../models/enhanced_best_model.joblib\"\n",
    "                if os.path.exists(extractor_path):\n",
    "                    saved_data = joblib.load(extractor_path)\n",
    "                    # Load the extractor architecture and weights\n",
    "                    from method_4_enhanced_feature_extractor_svm import EnhancedFeatureExtractor\n",
    "                    config = saved_data['extractor_config']\n",
    "                    self.image_backbone = EnhancedFeatureExtractor(\n",
    "                        config['backbone'], config['aggregation'], config['freeze_layers']\n",
    "                    )\n",
    "                    self.image_backbone.load_state_dict(saved_data['extractor'])\n",
    "                    self.feature_dim = self.image_backbone.feature_dim\n",
    "                    print(f\"   ‚úÖ Loaded fine-tuned {config['backbone']} with {config['aggregation']} aggregation\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(\"Enhanced extractor not found\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Could not load enhanced extractor: {e}\")\n",
    "                print(\"   ‚è© Using ResNet18 with fine-tuning capability...\")\n",
    "                self.image_backbone = models.resnet18(pretrained=True)\n",
    "                self.feature_dim = self.image_backbone.fc.in_features\n",
    "                self.image_backbone = nn.Sequential(*list(self.image_backbone.children())[:-1])\n",
    "                # Don't freeze - allow fine-tuning\n",
    "        else:\n",
    "            # Standard ResNet18 for comparison\n",
    "            self.image_backbone = models.resnet18(pretrained=True) \n",
    "            self.feature_dim = self.image_backbone.fc.in_features\n",
    "            self.image_backbone = nn.Sequential(*list(self.image_backbone.children())[:-1])\n",
    "        \n",
    "        # ENHANCEMENT: Advanced LBP processing branch\n",
    "        self.lbp_branch = nn.Sequential(\n",
    "            nn.Linear(lbp_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # ENHANCEMENT: Gated fusion mechanism\n",
    "        self.fusion_module = GatedFusionModule(self.feature_dim, 64, hidden_dim=256)\n",
    "        \n",
    "        # ENHANCEMENT: Advanced classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"üî¨ Enhanced Fusion CNN Architecture:\")\n",
    "        print(f\"   CNN features: {self.feature_dim}\")\n",
    "        print(f\"   LBP features: {lbp_dim} ‚Üí 64 (processed)\")\n",
    "        print(f\"   Fusion: Gated mechanism with cross-attention\")\n",
    "        print(f\"   Classifier: Advanced 3-layer with BatchNorm\")\n",
    "        print(f\"   Expected: Significant improvement over 37% baseline\")\n",
    "    \n",
    "    def forward(self, images, lbp_features):\n",
    "        # Process images through CNN backbone\n",
    "        batch_size, num_views, channels, height, width = images.shape\n",
    "        \n",
    "        if hasattr(self.image_backbone, 'forward') and hasattr(self.image_backbone, 'feature_dim'):\n",
    "            # Using enhanced feature extractor (handles multi-view internally)\n",
    "            cnn_features = self.image_backbone(images)\n",
    "        else:\n",
    "            # Standard CNN processing\n",
    "            images = images.view(-1, channels, height, width)\n",
    "            cnn_features = self.image_backbone(images)\n",
    "            cnn_features = cnn_features.view(cnn_features.size(0), -1)\n",
    "            \n",
    "            # Reshape back and aggregate across views\n",
    "            cnn_features = cnn_features.view(batch_size, num_views, -1)\n",
    "            cnn_features = torch.mean(cnn_features, dim=1)  # Average pooling\n",
    "        \n",
    "        # Process LBP features\n",
    "        lbp_processed = self.lbp_branch(lbp_features)\n",
    "        \n",
    "        # ENHANCEMENT: Gated fusion\n",
    "        fused_features = self.fusion_module(cnn_features, lbp_processed)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(fused_features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_cnn_params(self):\n",
    "        \"\"\"Get CNN backbone parameters for differential learning rates\"\"\"\n",
    "        return self.image_backbone.parameters()\n",
    "    \n",
    "    def get_fusion_params(self):\n",
    "        \"\"\"Get fusion-specific parameters (LBP branch + fusion + classifier)\"\"\"\n",
    "        fusion_params = []\n",
    "        fusion_params.extend(self.lbp_branch.parameters())\n",
    "        fusion_params.extend(self.fusion_module.parameters())\n",
    "        fusion_params.extend(self.classifier.parameters())\n",
    "        return fusion_params\n",
    "\n",
    "# Create the enhanced fusion model\n",
    "print(f\"\\nüèóÔ∏è Creating Enhanced Fusion Model...\")\n",
    "model = EnhancedFusionCNN(num_classes=num_classes, use_pretrained_extractor=True)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "cnn_params = sum(p.numel() for p in model.get_cnn_params())\n",
    "fusion_params = sum(p.numel() for p in model.get_fusion_params()) \n",
    "total_params = cnn_params + fusion_params\n",
    "\n",
    "print(f\"\\nüìä Enhanced Model Statistics:\")\n",
    "print(f\"   CNN parameters: {cnn_params:,}\")\n",
    "print(f\"   Fusion parameters: {fusion_params:,}\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Architecture: Gated fusion with cross-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5bc712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Enhanced Class Balancing:\n",
      "   Ash            :  56 samples ‚Üí weight 3.07 üî¥ HIGH\n",
      "   Beech          :  94 samples ‚Üí weight 0.88 üü¢ LOW\n",
      "   Douglas Fir    :  93 samples ‚Üí weight 0.53 üü¢ LOW\n",
      "   Oak            :  16 samples ‚Üí weight 3.51 üî¥ HIGH\n",
      "   Pine           :  65 samples ‚Üí weight 8.19 üî¥ HIGH\n",
      "   Red Oak        :  14 samples ‚Üí weight 0.76 üü¢ LOW\n",
      "   Spruce         :   6 samples ‚Üí weight 0.52 üü¢ LOW\n",
      "\n",
      "üéØ Enhanced Loss Function:\n",
      "   Type: Focal Loss (Œ≥=2.0)\n",
      "   Class weights: Applied to focus on minority classes\n",
      "   Expected: Better performance on Pine, Oak, Ash\n",
      "\n",
      "üîß Two-Stage Training Configuration:\n",
      "   Stage 1 (20 epochs):\n",
      "      Strategy: Train fusion components only (CNN frozen)\n",
      "      Optimizer: AdamW @ 0.001\n",
      "      Scheduler: CosineAnnealingLR\n",
      "      Focus: Learn optimal fusion of CNN + LBP features\n",
      "\n",
      "   Stage 2 (60 epochs):\n",
      "      Strategy: End-to-end fine-tuning (all parameters)\n",
      "      Optimizer: AdamW @ 5e-05 (differential rates)\n",
      "      Scheduler: CosineAnnealingWarmRestarts\n",
      "      Focus: Adapt CNN features to fusion model\n",
      "\n",
      "‚úÖ Enhanced Training Setup Complete:\n",
      "   Data loaders: Weighted sampling + pin_memory\n",
      "   Loss: Focal Loss with class weights\n",
      "   Strategy: Two-stage training\n",
      "   Monitoring: Advanced metrics tracking\n",
      "   Expected: 60%+ accuracy (vs 37% baseline)\n"
     ]
    }
   ],
   "source": [
    "# ENHANCEMENT: Advanced Training Setup with Two-Stage Strategy\n",
    "# Enhanced weighted sampling\n",
    "class_counts = Counter(train_labels_enc)\n",
    "total_samples = len(train_labels_enc)\n",
    "class_weights = {cls: total_samples / (num_classes * count) for cls, count in class_counts.items()}\n",
    "\n",
    "# Create sample weights and sampler for DataLoader  \n",
    "sample_weights = [class_weights[label] for label in train_labels_enc]\n",
    "weighted_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "print(\"‚öñÔ∏è Enhanced Class Balancing:\")\n",
    "for i, (class_name, count) in enumerate(class_counts.items()):\n",
    "    weight = class_weights[i]\n",
    "    status = \"üî¥ HIGH\" if weight > 2.0 else \"üü° MED\" if weight > 1.0 else \"üü¢ LOW\"  \n",
    "    print(f\"   {label_encoder.classes_[i]:<15}: {count:>3} samples ‚Üí weight {weight:.2f} {status}\")\n",
    "\n",
    "# Enhanced data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=weighted_sampler, pin_memory=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# ENHANCEMENT: Focal Loss with class weights for better minority class handling\n",
    "class_weights_tensor = torch.FloatTensor([class_weights[i] for i in range(num_classes)]).to(DEVICE)\n",
    "criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0, reduction='mean')\n",
    "\n",
    "print(f\"\\nüéØ Enhanced Loss Function:\")\n",
    "print(f\"   Type: Focal Loss (Œ≥=2.0)\")\n",
    "print(f\"   Class weights: Applied to focus on minority classes\")\n",
    "print(f\"   Expected: Better performance on Pine, Oak, Ash\")\n",
    "\n",
    "# ENHANCEMENT: Two-Stage Training Strategy\n",
    "def create_stage_optimizers(model, stage1_lr, stage2_lr):\n",
    "    \"\"\"Create optimizers for two-stage training\"\"\"\n",
    "    \n",
    "    # Stage 1: Only fusion components (CNN frozen)\n",
    "    fusion_params = list(model.get_fusion_params())\n",
    "    stage1_optimizer = optim.AdamW(fusion_params, lr=stage1_lr, weight_decay=1e-3)\n",
    "    \n",
    "    # Stage 2: All parameters (end-to-end fine-tuning)\n",
    "    all_params = list(model.parameters())\n",
    "    stage2_optimizer = optim.AdamW([\n",
    "        {'params': model.get_cnn_params(), 'lr': stage2_lr * 0.1},  # Very low LR for CNN\n",
    "        {'params': model.get_fusion_params(), 'lr': stage2_lr}       # Higher LR for fusion components\n",
    "    ], weight_decay=1e-4)\n",
    "    \n",
    "    return stage1_optimizer, stage2_optimizer\n",
    "\n",
    "# Create optimizers for both stages\n",
    "stage1_optimizer, stage2_optimizer = create_stage_optimizers(model, STAGE1_LR, STAGE2_LR)\n",
    "\n",
    "# Create schedulers\n",
    "stage1_scheduler = optim.lr_scheduler.CosineAnnealingLR(stage1_optimizer, T_max=STAGE1_EPOCHS, eta_min=1e-6)\n",
    "stage2_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(stage2_optimizer, T_0=15, T_mult=2, eta_min=1e-7)\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels).item()\n",
    "\n",
    "print(f\"\\nüîß Two-Stage Training Configuration:\")\n",
    "print(f\"   Stage 1 ({STAGE1_EPOCHS} epochs):\")\n",
    "print(f\"      Strategy: Train fusion components only (CNN frozen)\")\n",
    "print(f\"      Optimizer: AdamW @ {STAGE1_LR}\")\n",
    "print(f\"      Scheduler: CosineAnnealingLR\") \n",
    "print(f\"      Focus: Learn optimal fusion of CNN + LBP features\")\n",
    "print(f\"\\n   Stage 2 ({STAGE2_EPOCHS} epochs):\")\n",
    "print(f\"      Strategy: End-to-end fine-tuning (all parameters)\")\n",
    "print(f\"      Optimizer: AdamW @ {STAGE2_LR} (differential rates)\")\n",
    "print(f\"      Scheduler: CosineAnnealingWarmRestarts\")\n",
    "print(f\"      Focus: Adapt CNN features to fusion model\")\n",
    "\n",
    "# ENHANCEMENT: Advanced monitoring setup\n",
    "class TrainingMonitor:\n",
    "    \"\"\"Enhanced training monitor with stage tracking\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.history = {\n",
    "            'stage1': {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []},\n",
    "            'stage2': {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []},\n",
    "            'epoch_times': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_model_state = None\n",
    "        self.current_stage = 1\n",
    "    \n",
    "    def set_stage(self, stage):\n",
    "        self.current_stage = stage\n",
    "        print(f\"\\nüîÑ SWITCHING TO STAGE {stage}\")\n",
    "        \n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc, lr, epoch_time, model_state=None):\n",
    "        stage_key = f'stage{self.current_stage}'\n",
    "        self.history[stage_key]['train_loss'].append(train_loss)\n",
    "        self.history[stage_key]['train_acc'].append(train_acc)\n",
    "        self.history[stage_key]['val_loss'].append(val_loss)\n",
    "        self.history[stage_key]['val_acc'].append(val_acc)\n",
    "        self.history['epoch_times'].append(epoch_time)\n",
    "        self.history['learning_rates'].append(lr)\n",
    "        \n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            if model_state is not None:\n",
    "                self.best_model_state = model_state\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "monitor = TrainingMonitor()\n",
    "\n",
    "print(f\"\\n‚úÖ Enhanced Training Setup Complete:\")\n",
    "print(f\"   Data loaders: Weighted sampling + pin_memory\")\n",
    "print(f\"   Loss: Focal Loss with class weights\")\n",
    "print(f\"   Strategy: Two-stage training\")\n",
    "print(f\"   Monitoring: Advanced metrics tracking\")\n",
    "print(f\"   Expected: 60%+ accuracy (vs 37% baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "817d357c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Beginning Enhanced Fusion Training...\n",
      "üöÄ ENHANCED TWO-STAGE FUSION TRAINING\n",
      "================================================================================\n",
      "üî• STAGE 1: Training Fusion Components (CNN Frozen)\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ SWITCHING TO STAGE 1\n",
      "   Stage1 Epoch  1 | Batch   0 | Loss: 3.7188\n",
      "   Stage1 Epoch  1 | Batch   0 | Loss: 3.7188\n",
      "   Stage1 Epoch  1 | Batch  20 | Loss: 4.3251\n",
      "   Stage1 Epoch  1 | Batch  20 | Loss: 4.3251\n",
      "   S1 Epoch  1/20: Train=0.151, Val=0.035 ‚≠ê BEST\n",
      "   S1 Epoch  1/20: Train=0.151, Val=0.035 ‚≠ê BEST\n",
      "   Stage1 Epoch  2 | Batch   0 | Loss: 1.8757\n",
      "   Stage1 Epoch  2 | Batch   0 | Loss: 1.8757\n",
      "   Stage1 Epoch  2 | Batch  20 | Loss: 4.1951\n",
      "   Stage1 Epoch  2 | Batch  20 | Loss: 4.1951\n",
      "   S1 Epoch  2/20: Train=0.224, Val=0.058 ‚≠ê BEST\n",
      "   S1 Epoch  2/20: Train=0.224, Val=0.058 ‚≠ê BEST\n",
      "   Stage1 Epoch  3 | Batch   0 | Loss: 3.7964\n",
      "   Stage1 Epoch  3 | Batch   0 | Loss: 3.7964\n",
      "   Stage1 Epoch  3 | Batch  20 | Loss: 3.4657\n",
      "   Stage1 Epoch  3 | Batch  20 | Loss: 3.4657\n",
      "   S1 Epoch  3/20: Train=0.192, Val=0.047 \n",
      "   S1 Epoch  3/20: Train=0.192, Val=0.047 \n",
      "   Stage1 Epoch  4 | Batch   0 | Loss: 2.9561\n",
      "   Stage1 Epoch  4 | Batch   0 | Loss: 2.9561\n",
      "   Stage1 Epoch  4 | Batch  20 | Loss: 2.3100\n",
      "   Stage1 Epoch  4 | Batch  20 | Loss: 2.3100\n",
      "   S1 Epoch  4/20: Train=0.259, Val=0.093 ‚≠ê BEST\n",
      "   S1 Epoch  4/20: Train=0.259, Val=0.093 ‚≠ê BEST\n",
      "   Stage1 Epoch  5 | Batch   0 | Loss: 3.0230\n",
      "   Stage1 Epoch  5 | Batch   0 | Loss: 3.0230\n",
      "   Stage1 Epoch  5 | Batch  20 | Loss: 2.2667\n",
      "   Stage1 Epoch  5 | Batch  20 | Loss: 2.2667\n",
      "   S1 Epoch  5/20: Train=0.273, Val=0.151 ‚≠ê BEST\n",
      "   S1 Epoch  5/20: Train=0.273, Val=0.151 ‚≠ê BEST\n",
      "   Stage1 Epoch  6 | Batch   0 | Loss: 3.2778\n",
      "   Stage1 Epoch  6 | Batch   0 | Loss: 3.2778\n",
      "   Stage1 Epoch  6 | Batch  20 | Loss: 2.1628\n",
      "   Stage1 Epoch  6 | Batch  20 | Loss: 2.1628\n",
      "   S1 Epoch  6/20: Train=0.305, Val=0.047 \n",
      "   S1 Epoch  6/20: Train=0.305, Val=0.047 \n",
      "   Stage1 Epoch  7 | Batch   0 | Loss: 1.0286\n",
      "   Stage1 Epoch  7 | Batch   0 | Loss: 1.0286\n",
      "   Stage1 Epoch  7 | Batch  20 | Loss: 1.7568\n",
      "   Stage1 Epoch  7 | Batch  20 | Loss: 1.7568\n",
      "   S1 Epoch  7/20: Train=0.334, Val=0.047 \n",
      "   S1 Epoch  7/20: Train=0.334, Val=0.047 \n",
      "   Stage1 Epoch  8 | Batch   0 | Loss: 1.5338\n",
      "   Stage1 Epoch  8 | Batch   0 | Loss: 1.5338\n",
      "   Stage1 Epoch  8 | Batch  20 | Loss: 1.6911\n",
      "   Stage1 Epoch  8 | Batch  20 | Loss: 1.6911\n",
      "   S1 Epoch  8/20: Train=0.360, Val=0.047 \n",
      "   S1 Epoch  8/20: Train=0.360, Val=0.047 \n",
      "   Stage1 Epoch  9 | Batch   0 | Loss: 1.1749\n",
      "   Stage1 Epoch  9 | Batch   0 | Loss: 1.1749\n",
      "   Stage1 Epoch  9 | Batch  20 | Loss: 0.9523\n",
      "   Stage1 Epoch  9 | Batch  20 | Loss: 0.9523\n",
      "   S1 Epoch  9/20: Train=0.384, Val=0.058 \n",
      "   S1 Epoch  9/20: Train=0.384, Val=0.058 \n",
      "   Stage1 Epoch 10 | Batch   0 | Loss: 1.7209\n",
      "   Stage1 Epoch 10 | Batch   0 | Loss: 1.7209\n",
      "   Stage1 Epoch 10 | Batch  20 | Loss: 1.7413\n",
      "   Stage1 Epoch 10 | Batch  20 | Loss: 1.7413\n",
      "   S1 Epoch 10/20: Train=0.375, Val=0.070 \n",
      "   S1 Epoch 10/20: Train=0.375, Val=0.070 \n",
      "   Stage1 Epoch 11 | Batch   0 | Loss: 1.8028\n",
      "   Stage1 Epoch 11 | Batch   0 | Loss: 1.8028\n",
      "   Stage1 Epoch 11 | Batch  20 | Loss: 1.5896\n",
      "   Stage1 Epoch 11 | Batch  20 | Loss: 1.5896\n",
      "   S1 Epoch 11/20: Train=0.355, Val=0.058 \n",
      "   S1 Epoch 11/20: Train=0.355, Val=0.058 \n",
      "   Stage1 Epoch 12 | Batch   0 | Loss: 1.8150\n",
      "   Stage1 Epoch 12 | Batch   0 | Loss: 1.8150\n",
      "   Stage1 Epoch 12 | Batch  20 | Loss: 2.9434\n",
      "   Stage1 Epoch 12 | Batch  20 | Loss: 2.9434\n",
      "   S1 Epoch 12/20: Train=0.404, Val=0.047 \n",
      "   S1 Epoch 12/20: Train=0.404, Val=0.047 \n",
      "   Stage1 Epoch 13 | Batch   0 | Loss: 0.7508\n",
      "   Stage1 Epoch 13 | Batch   0 | Loss: 0.7508\n",
      "   Stage1 Epoch 13 | Batch  20 | Loss: 0.8397\n",
      "   Stage1 Epoch 13 | Batch  20 | Loss: 0.8397\n",
      "   S1 Epoch 13/20: Train=0.419, Val=0.093 \n",
      "   S1 Epoch 13/20: Train=0.419, Val=0.093 \n",
      "   Stage1 Epoch 14 | Batch   0 | Loss: 1.5355\n",
      "   Stage1 Epoch 14 | Batch   0 | Loss: 1.5355\n",
      "   Stage1 Epoch 14 | Batch  20 | Loss: 0.8602\n",
      "   Stage1 Epoch 14 | Batch  20 | Loss: 0.8602\n",
      "   S1 Epoch 14/20: Train=0.422, Val=0.070 \n",
      "   S1 Epoch 14/20: Train=0.422, Val=0.070 \n",
      "   Stage1 Epoch 15 | Batch   0 | Loss: 0.8371\n",
      "   Stage1 Epoch 15 | Batch   0 | Loss: 0.8371\n",
      "   Stage1 Epoch 15 | Batch  20 | Loss: 0.7489\n",
      "   Stage1 Epoch 15 | Batch  20 | Loss: 0.7489\n",
      "   S1 Epoch 15/20: Train=0.427, Val=0.081 \n",
      "   S1 Epoch 15/20: Train=0.427, Val=0.081 \n",
      "   Stage1 Epoch 16 | Batch   0 | Loss: 1.8006\n",
      "   Stage1 Epoch 16 | Batch   0 | Loss: 1.8006\n",
      "   Stage1 Epoch 16 | Batch  20 | Loss: 0.8204\n",
      "   Stage1 Epoch 16 | Batch  20 | Loss: 0.8204\n",
      "   S1 Epoch 16/20: Train=0.453, Val=0.058 \n",
      "   S1 Epoch 16/20: Train=0.453, Val=0.058 \n",
      "   Stage1 Epoch 17 | Batch   0 | Loss: 0.8960\n",
      "   Stage1 Epoch 17 | Batch   0 | Loss: 0.8960\n",
      "   Stage1 Epoch 17 | Batch  20 | Loss: 1.0330\n",
      "   Stage1 Epoch 17 | Batch  20 | Loss: 1.0330\n",
      "   S1 Epoch 17/20: Train=0.392, Val=0.081 \n",
      "   S1 Epoch 17/20: Train=0.392, Val=0.081 \n",
      "   Stage1 Epoch 18 | Batch   0 | Loss: 1.9801\n",
      "   Stage1 Epoch 18 | Batch   0 | Loss: 1.9801\n",
      "   Stage1 Epoch 18 | Batch  20 | Loss: 0.9029\n",
      "   Stage1 Epoch 18 | Batch  20 | Loss: 0.9029\n",
      "   S1 Epoch 18/20: Train=0.422, Val=0.105 \n",
      "   S1 Epoch 18/20: Train=0.422, Val=0.105 \n",
      "   Stage1 Epoch 19 | Batch   0 | Loss: 1.1259\n",
      "   Stage1 Epoch 19 | Batch   0 | Loss: 1.1259\n",
      "   Stage1 Epoch 19 | Batch  20 | Loss: 0.7564\n",
      "   Stage1 Epoch 19 | Batch  20 | Loss: 0.7564\n",
      "   S1 Epoch 19/20: Train=0.468, Val=0.093 \n",
      "   S1 Epoch 19/20: Train=0.468, Val=0.093 \n",
      "   Stage1 Epoch 20 | Batch   0 | Loss: 1.4850\n",
      "   Stage1 Epoch 20 | Batch   0 | Loss: 1.4850\n",
      "   Stage1 Epoch 20 | Batch  20 | Loss: 0.6410\n",
      "   Stage1 Epoch 20 | Batch  20 | Loss: 0.6410\n",
      "   S1 Epoch 20/20: Train=0.424, Val=0.093 \n",
      "‚úÖ Stage 1 Complete: Best Val Acc = 0.151\n",
      "\n",
      "üî• STAGE 2: End-to-End Fine-Tuning\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ SWITCHING TO STAGE 2\n",
      "   S1 Epoch 20/20: Train=0.424, Val=0.093 \n",
      "‚úÖ Stage 1 Complete: Best Val Acc = 0.151\n",
      "\n",
      "üî• STAGE 2: End-to-End Fine-Tuning\n",
      "--------------------------------------------------\n",
      "\n",
      "üîÑ SWITCHING TO STAGE 2\n",
      "   Stage2 Epoch  1 | Batch   0 | Loss: 1.6630\n",
      "   Stage2 Epoch  1 | Batch   0 | Loss: 1.6630\n",
      "   Stage2 Epoch  1 | Batch  20 | Loss: 0.4435\n",
      "   Stage2 Epoch  1 | Batch  20 | Loss: 0.4435\n",
      "   S2 Epoch  1/60: Train=0.401, Val=0.093 \n",
      "   S2 Epoch  1/60: Train=0.401, Val=0.093 \n",
      "   Stage2 Epoch  2 | Batch   0 | Loss: 0.4819\n",
      "   Stage2 Epoch  2 | Batch   0 | Loss: 0.4819\n",
      "   Stage2 Epoch  2 | Batch  20 | Loss: 0.7724\n",
      "   Stage2 Epoch  2 | Batch  20 | Loss: 0.7724\n",
      "   S2 Epoch  2/60: Train=0.416, Val=0.070 \n",
      "   S2 Epoch  2/60: Train=0.416, Val=0.070 \n",
      "   Stage2 Epoch  3 | Batch   0 | Loss: 0.6961\n",
      "   Stage2 Epoch  3 | Batch   0 | Loss: 0.6961\n",
      "   Stage2 Epoch  3 | Batch  20 | Loss: 0.8863\n",
      "   Stage2 Epoch  3 | Batch  20 | Loss: 0.8863\n",
      "   S2 Epoch  3/60: Train=0.436, Val=0.140 \n",
      "   S2 Epoch  3/60: Train=0.436, Val=0.140 \n",
      "   Stage2 Epoch  4 | Batch   0 | Loss: 0.5970\n",
      "   Stage2 Epoch  4 | Batch   0 | Loss: 0.5970\n",
      "   Stage2 Epoch  4 | Batch  20 | Loss: 0.6570\n",
      "   Stage2 Epoch  4 | Batch  20 | Loss: 0.6570\n",
      "   S2 Epoch  4/60: Train=0.424, Val=0.151 \n",
      "   S2 Epoch  4/60: Train=0.424, Val=0.151 \n",
      "   Stage2 Epoch  5 | Batch   0 | Loss: 0.3732\n",
      "   Stage2 Epoch  5 | Batch   0 | Loss: 0.3732\n",
      "   Stage2 Epoch  5 | Batch  20 | Loss: 0.5766\n",
      "   Stage2 Epoch  5 | Batch  20 | Loss: 0.5766\n",
      "   S2 Epoch  5/60: Train=0.433, Val=0.174 ‚≠ê BEST\n",
      "   S2 Epoch  5/60: Train=0.433, Val=0.174 ‚≠ê BEST\n",
      "   Stage2 Epoch  6 | Batch   0 | Loss: 0.3962\n",
      "   Stage2 Epoch  6 | Batch   0 | Loss: 0.3962\n",
      "   Stage2 Epoch  6 | Batch  20 | Loss: 0.8408\n",
      "   Stage2 Epoch  6 | Batch  20 | Loss: 0.8408\n",
      "   S2 Epoch  6/60: Train=0.471, Val=0.140 \n",
      "   S2 Epoch  6/60: Train=0.471, Val=0.140 \n",
      "   Stage2 Epoch  7 | Batch   0 | Loss: 1.0541\n",
      "   Stage2 Epoch  7 | Batch   0 | Loss: 1.0541\n",
      "   Stage2 Epoch  7 | Batch  20 | Loss: 0.5907\n",
      "   Stage2 Epoch  7 | Batch  20 | Loss: 0.5907\n",
      "   S2 Epoch  7/60: Train=0.456, Val=0.209 ‚≠ê BEST\n",
      "   S2 Epoch  7/60: Train=0.456, Val=0.209 ‚≠ê BEST\n",
      "   Stage2 Epoch  8 | Batch   0 | Loss: 0.6424\n",
      "   Stage2 Epoch  8 | Batch   0 | Loss: 0.6424\n",
      "   Stage2 Epoch  8 | Batch  20 | Loss: 0.3660\n",
      "   Stage2 Epoch  8 | Batch  20 | Loss: 0.3660\n",
      "   S2 Epoch  8/60: Train=0.480, Val=0.174 \n",
      "   S2 Epoch  8/60: Train=0.480, Val=0.174 \n",
      "   Stage2 Epoch  9 | Batch   0 | Loss: 0.4699\n",
      "   Stage2 Epoch  9 | Batch   0 | Loss: 0.4699\n",
      "   Stage2 Epoch  9 | Batch  20 | Loss: 0.5450\n",
      "   Stage2 Epoch  9 | Batch  20 | Loss: 0.5450\n",
      "   S2 Epoch  9/60: Train=0.468, Val=0.140 \n",
      "   S2 Epoch  9/60: Train=0.468, Val=0.140 \n",
      "   Stage2 Epoch 10 | Batch   0 | Loss: 0.5878\n",
      "   Stage2 Epoch 10 | Batch   0 | Loss: 0.5878\n",
      "   Stage2 Epoch 10 | Batch  20 | Loss: 0.5090\n",
      "   Stage2 Epoch 10 | Batch  20 | Loss: 0.5090\n",
      "   S2 Epoch 10/60: Train=0.477, Val=0.186 \n",
      "   S2 Epoch 10/60: Train=0.477, Val=0.186 \n",
      "   Stage2 Epoch 11 | Batch   0 | Loss: 0.9873\n",
      "   Stage2 Epoch 11 | Batch   0 | Loss: 0.9873\n",
      "   Stage2 Epoch 11 | Batch  20 | Loss: 0.3675\n",
      "   Stage2 Epoch 11 | Batch  20 | Loss: 0.3675\n",
      "   S2 Epoch 11/60: Train=0.526, Val=0.163 \n",
      "   S2 Epoch 11/60: Train=0.526, Val=0.163 \n",
      "   Stage2 Epoch 12 | Batch   0 | Loss: 0.5949\n",
      "   Stage2 Epoch 12 | Batch   0 | Loss: 0.5949\n",
      "   Stage2 Epoch 12 | Batch  20 | Loss: 0.6960\n",
      "   Stage2 Epoch 12 | Batch  20 | Loss: 0.6960\n",
      "   S2 Epoch 12/60: Train=0.456, Val=0.186 \n",
      "   S2 Epoch 12/60: Train=0.456, Val=0.186 \n",
      "   Stage2 Epoch 13 | Batch   0 | Loss: 0.4054\n",
      "   Stage2 Epoch 13 | Batch   0 | Loss: 0.4054\n",
      "   Stage2 Epoch 13 | Batch  20 | Loss: 3.1153\n",
      "   Stage2 Epoch 13 | Batch  20 | Loss: 3.1153\n",
      "   S2 Epoch 13/60: Train=0.517, Val=0.186 \n",
      "   S2 Epoch 13/60: Train=0.517, Val=0.186 \n",
      "   Stage2 Epoch 14 | Batch   0 | Loss: 0.9527\n",
      "   Stage2 Epoch 14 | Batch   0 | Loss: 0.9527\n",
      "   Stage2 Epoch 14 | Batch  20 | Loss: 0.6272\n",
      "   Stage2 Epoch 14 | Batch  20 | Loss: 0.6272\n",
      "   S2 Epoch 14/60: Train=0.488, Val=0.221 ‚≠ê BEST\n",
      "   S2 Epoch 14/60: Train=0.488, Val=0.221 ‚≠ê BEST\n",
      "   Stage2 Epoch 15 | Batch   0 | Loss: 0.6611\n",
      "   Stage2 Epoch 15 | Batch   0 | Loss: 0.6611\n",
      "   Stage2 Epoch 15 | Batch  20 | Loss: 0.4789\n",
      "   Stage2 Epoch 15 | Batch  20 | Loss: 0.4789\n",
      "   S2 Epoch 15/60: Train=0.480, Val=0.174 \n",
      "   S2 Epoch 15/60: Train=0.480, Val=0.174 \n",
      "   Stage2 Epoch 16 | Batch   0 | Loss: 0.6116\n",
      "   Stage2 Epoch 16 | Batch   0 | Loss: 0.6116\n",
      "   Stage2 Epoch 16 | Batch  20 | Loss: 0.8648\n",
      "   Stage2 Epoch 16 | Batch  20 | Loss: 0.8648\n",
      "   S2 Epoch 16/60: Train=0.477, Val=0.151 \n",
      "   S2 Epoch 16/60: Train=0.477, Val=0.151 \n",
      "   Stage2 Epoch 17 | Batch   0 | Loss: 0.7873\n",
      "   Stage2 Epoch 17 | Batch   0 | Loss: 0.7873\n",
      "   Stage2 Epoch 17 | Batch  20 | Loss: 0.5608\n",
      "   Stage2 Epoch 17 | Batch  20 | Loss: 0.5608\n",
      "   S2 Epoch 17/60: Train=0.515, Val=0.186 \n",
      "   S2 Epoch 17/60: Train=0.515, Val=0.186 \n",
      "   Stage2 Epoch 18 | Batch   0 | Loss: 0.6049\n",
      "   Stage2 Epoch 18 | Batch   0 | Loss: 0.6049\n",
      "   Stage2 Epoch 18 | Batch  20 | Loss: 0.7553\n",
      "   Stage2 Epoch 18 | Batch  20 | Loss: 0.7553\n",
      "   S2 Epoch 18/60: Train=0.459, Val=0.267 ‚≠ê BEST\n",
      "   S2 Epoch 18/60: Train=0.459, Val=0.267 ‚≠ê BEST\n",
      "   Stage2 Epoch 19 | Batch   0 | Loss: 0.3561\n",
      "   Stage2 Epoch 19 | Batch   0 | Loss: 0.3561\n",
      "   Stage2 Epoch 19 | Batch  20 | Loss: 1.6836\n",
      "   Stage2 Epoch 19 | Batch  20 | Loss: 1.6836\n",
      "   S2 Epoch 19/60: Train=0.416, Val=0.244 \n",
      "   S2 Epoch 19/60: Train=0.416, Val=0.244 \n",
      "   Stage2 Epoch 20 | Batch   0 | Loss: 0.2784\n",
      "   Stage2 Epoch 20 | Batch   0 | Loss: 0.2784\n",
      "   Stage2 Epoch 20 | Batch  20 | Loss: 0.3666\n",
      "   Stage2 Epoch 20 | Batch  20 | Loss: 0.3666\n",
      "   S2 Epoch 20/60: Train=0.488, Val=0.198 \n",
      "   S2 Epoch 20/60: Train=0.488, Val=0.198 \n",
      "   Stage2 Epoch 21 | Batch   0 | Loss: 0.5638\n",
      "   Stage2 Epoch 21 | Batch   0 | Loss: 0.5638\n",
      "   Stage2 Epoch 21 | Batch  20 | Loss: 0.5466\n",
      "   Stage2 Epoch 21 | Batch  20 | Loss: 0.5466\n",
      "   S2 Epoch 21/60: Train=0.462, Val=0.233 \n",
      "   S2 Epoch 21/60: Train=0.462, Val=0.233 \n",
      "   Stage2 Epoch 22 | Batch   0 | Loss: 0.5499\n",
      "   Stage2 Epoch 22 | Batch   0 | Loss: 0.5499\n",
      "   Stage2 Epoch 22 | Batch  20 | Loss: 1.1709\n",
      "   Stage2 Epoch 22 | Batch  20 | Loss: 1.1709\n",
      "   S2 Epoch 22/60: Train=0.480, Val=0.186 \n",
      "   S2 Epoch 22/60: Train=0.480, Val=0.186 \n",
      "   Stage2 Epoch 23 | Batch   0 | Loss: 0.4454\n",
      "   Stage2 Epoch 23 | Batch   0 | Loss: 0.4454\n",
      "   Stage2 Epoch 23 | Batch  20 | Loss: 0.6553\n",
      "   Stage2 Epoch 23 | Batch  20 | Loss: 0.6553\n",
      "   S2 Epoch 23/60: Train=0.532, Val=0.233 \n",
      "   S2 Epoch 23/60: Train=0.532, Val=0.233 \n",
      "   Stage2 Epoch 24 | Batch   0 | Loss: 0.2935\n",
      "   Stage2 Epoch 24 | Batch   0 | Loss: 0.2935\n",
      "   Stage2 Epoch 24 | Batch  20 | Loss: 0.9512\n",
      "   Stage2 Epoch 24 | Batch  20 | Loss: 0.9512\n",
      "   S2 Epoch 24/60: Train=0.578, Val=0.256 \n",
      "   S2 Epoch 24/60: Train=0.578, Val=0.256 \n",
      "   Stage2 Epoch 25 | Batch   0 | Loss: 0.5171\n",
      "   Stage2 Epoch 25 | Batch   0 | Loss: 0.5171\n",
      "   Stage2 Epoch 25 | Batch  20 | Loss: 0.3694\n",
      "   Stage2 Epoch 25 | Batch  20 | Loss: 0.3694\n",
      "   S2 Epoch 25/60: Train=0.535, Val=0.314 ‚≠ê BEST\n",
      "   S2 Epoch 25/60: Train=0.535, Val=0.314 ‚≠ê BEST\n",
      "   Stage2 Epoch 26 | Batch   0 | Loss: 0.4936\n",
      "   Stage2 Epoch 26 | Batch   0 | Loss: 0.4936\n",
      "   Stage2 Epoch 26 | Batch  20 | Loss: 0.6097\n",
      "   Stage2 Epoch 26 | Batch  20 | Loss: 0.6097\n",
      "   S2 Epoch 26/60: Train=0.567, Val=0.267 \n",
      "   S2 Epoch 26/60: Train=0.567, Val=0.267 \n",
      "   Stage2 Epoch 27 | Batch   0 | Loss: 1.4085\n",
      "   Stage2 Epoch 27 | Batch   0 | Loss: 1.4085\n",
      "   Stage2 Epoch 27 | Batch  20 | Loss: 0.5187\n",
      "   Stage2 Epoch 27 | Batch  20 | Loss: 0.5187\n",
      "   S2 Epoch 27/60: Train=0.526, Val=0.314 \n",
      "   S2 Epoch 27/60: Train=0.526, Val=0.314 \n",
      "   Stage2 Epoch 28 | Batch   0 | Loss: 0.5706\n",
      "   Stage2 Epoch 28 | Batch   0 | Loss: 0.5706\n",
      "   Stage2 Epoch 28 | Batch  20 | Loss: 0.3109\n",
      "   Stage2 Epoch 28 | Batch  20 | Loss: 0.3109\n",
      "   S2 Epoch 28/60: Train=0.512, Val=0.302 \n",
      "   S2 Epoch 28/60: Train=0.512, Val=0.302 \n",
      "   Stage2 Epoch 29 | Batch   0 | Loss: 0.5405\n",
      "   Stage2 Epoch 29 | Batch   0 | Loss: 0.5405\n",
      "   Stage2 Epoch 29 | Batch  20 | Loss: 0.3239\n",
      "   Stage2 Epoch 29 | Batch  20 | Loss: 0.3239\n",
      "   S2 Epoch 29/60: Train=0.593, Val=0.256 \n",
      "   S2 Epoch 29/60: Train=0.593, Val=0.256 \n",
      "   Stage2 Epoch 30 | Batch   0 | Loss: 0.6319\n",
      "   Stage2 Epoch 30 | Batch   0 | Loss: 0.6319\n",
      "   Stage2 Epoch 30 | Batch  20 | Loss: 0.2910\n",
      "   Stage2 Epoch 30 | Batch  20 | Loss: 0.2910\n",
      "   S2 Epoch 30/60: Train=0.581, Val=0.349 ‚≠ê BEST\n",
      "   S2 Epoch 30/60: Train=0.581, Val=0.349 ‚≠ê BEST\n",
      "   Stage2 Epoch 31 | Batch   0 | Loss: 1.0945\n",
      "   Stage2 Epoch 31 | Batch   0 | Loss: 1.0945\n",
      "   Stage2 Epoch 31 | Batch  20 | Loss: 0.4410\n",
      "   Stage2 Epoch 31 | Batch  20 | Loss: 0.4410\n",
      "   S2 Epoch 31/60: Train=0.509, Val=0.326 \n",
      "   S2 Epoch 31/60: Train=0.509, Val=0.326 \n",
      "   Stage2 Epoch 32 | Batch   0 | Loss: 0.8183\n",
      "   Stage2 Epoch 32 | Batch   0 | Loss: 0.8183\n",
      "   Stage2 Epoch 32 | Batch  20 | Loss: 0.2825\n",
      "   Stage2 Epoch 32 | Batch  20 | Loss: 0.2825\n",
      "   S2 Epoch 32/60: Train=0.529, Val=0.256 \n",
      "   S2 Epoch 32/60: Train=0.529, Val=0.256 \n",
      "   Stage2 Epoch 33 | Batch   0 | Loss: 0.4593\n",
      "   Stage2 Epoch 33 | Batch   0 | Loss: 0.4593\n",
      "   Stage2 Epoch 33 | Batch  20 | Loss: 0.8297\n",
      "   Stage2 Epoch 33 | Batch  20 | Loss: 0.8297\n",
      "   S2 Epoch 33/60: Train=0.561, Val=0.326 \n",
      "   S2 Epoch 33/60: Train=0.561, Val=0.326 \n",
      "   Stage2 Epoch 34 | Batch   0 | Loss: 0.6049\n",
      "   Stage2 Epoch 34 | Batch   0 | Loss: 0.6049\n",
      "   Stage2 Epoch 34 | Batch  20 | Loss: 0.3680\n",
      "   Stage2 Epoch 34 | Batch  20 | Loss: 0.3680\n",
      "   S2 Epoch 34/60: Train=0.509, Val=0.360 ‚≠ê BEST\n",
      "   S2 Epoch 34/60: Train=0.509, Val=0.360 ‚≠ê BEST\n",
      "   Stage2 Epoch 35 | Batch   0 | Loss: 0.4416\n",
      "   Stage2 Epoch 35 | Batch   0 | Loss: 0.4416\n",
      "   Stage2 Epoch 35 | Batch  20 | Loss: 0.3508\n",
      "   Stage2 Epoch 35 | Batch  20 | Loss: 0.3508\n",
      "   S2 Epoch 35/60: Train=0.532, Val=0.337 \n",
      "   S2 Epoch 35/60: Train=0.532, Val=0.337 \n",
      "   Stage2 Epoch 36 | Batch   0 | Loss: 0.5397\n",
      "   Stage2 Epoch 36 | Batch   0 | Loss: 0.5397\n",
      "   Stage2 Epoch 36 | Batch  20 | Loss: 0.2894\n",
      "   Stage2 Epoch 36 | Batch  20 | Loss: 0.2894\n",
      "   S2 Epoch 36/60: Train=0.616, Val=0.337 \n",
      "   S2 Epoch 36/60: Train=0.616, Val=0.337 \n",
      "   Stage2 Epoch 37 | Batch   0 | Loss: 0.4799\n",
      "   Stage2 Epoch 37 | Batch   0 | Loss: 0.4799\n",
      "   Stage2 Epoch 37 | Batch  20 | Loss: 0.2464\n",
      "   Stage2 Epoch 37 | Batch  20 | Loss: 0.2464\n",
      "   S2 Epoch 37/60: Train=0.549, Val=0.372 ‚≠ê BEST\n",
      "   S2 Epoch 37/60: Train=0.549, Val=0.372 ‚≠ê BEST\n",
      "   Stage2 Epoch 38 | Batch   0 | Loss: 0.3657\n",
      "   Stage2 Epoch 38 | Batch   0 | Loss: 0.3657\n",
      "   Stage2 Epoch 38 | Batch  20 | Loss: 0.4072\n",
      "   Stage2 Epoch 38 | Batch  20 | Loss: 0.4072\n",
      "   S2 Epoch 38/60: Train=0.625, Val=0.407 ‚≠ê BEST\n",
      "   S2 Epoch 38/60: Train=0.625, Val=0.407 ‚≠ê BEST\n",
      "   Stage2 Epoch 39 | Batch   0 | Loss: 0.4140\n",
      "   Stage2 Epoch 39 | Batch   0 | Loss: 0.4140\n",
      "   Stage2 Epoch 39 | Batch  20 | Loss: 1.0012\n",
      "   Stage2 Epoch 39 | Batch  20 | Loss: 1.0012\n",
      "   S2 Epoch 39/60: Train=0.622, Val=0.360 \n",
      "   S2 Epoch 39/60: Train=0.622, Val=0.360 \n",
      "   Stage2 Epoch 40 | Batch   0 | Loss: 0.3950\n",
      "   Stage2 Epoch 40 | Batch   0 | Loss: 0.3950\n",
      "   Stage2 Epoch 40 | Batch  20 | Loss: 0.2449\n",
      "   Stage2 Epoch 40 | Batch  20 | Loss: 0.2449\n",
      "   S2 Epoch 40/60: Train=0.567, Val=0.407 \n",
      "   S2 Epoch 40/60: Train=0.567, Val=0.407 \n",
      "   Stage2 Epoch 41 | Batch   0 | Loss: 0.3148\n",
      "   Stage2 Epoch 41 | Batch   0 | Loss: 0.3148\n",
      "   Stage2 Epoch 41 | Batch  20 | Loss: 0.3772\n",
      "   Stage2 Epoch 41 | Batch  20 | Loss: 0.3772\n",
      "   S2 Epoch 41/60: Train=0.584, Val=0.337 \n",
      "   S2 Epoch 41/60: Train=0.584, Val=0.337 \n",
      "   Stage2 Epoch 42 | Batch   0 | Loss: 0.1850\n",
      "   Stage2 Epoch 42 | Batch   0 | Loss: 0.1850\n",
      "   Stage2 Epoch 42 | Batch  20 | Loss: 0.4987\n",
      "   Stage2 Epoch 42 | Batch  20 | Loss: 0.4987\n",
      "   S2 Epoch 42/60: Train=0.529, Val=0.372 \n",
      "   S2 Epoch 42/60: Train=0.529, Val=0.372 \n",
      "   Stage2 Epoch 43 | Batch   0 | Loss: 0.6287\n",
      "   Stage2 Epoch 43 | Batch   0 | Loss: 0.6287\n",
      "   Stage2 Epoch 43 | Batch  20 | Loss: 0.2754\n",
      "   Stage2 Epoch 43 | Batch  20 | Loss: 0.2754\n",
      "   S2 Epoch 43/60: Train=0.616, Val=0.395 \n",
      "   S2 Epoch 43/60: Train=0.616, Val=0.395 \n",
      "   Stage2 Epoch 44 | Batch   0 | Loss: 0.2054\n",
      "   Stage2 Epoch 44 | Batch   0 | Loss: 0.2054\n",
      "   Stage2 Epoch 44 | Batch  20 | Loss: 0.9778\n",
      "   Stage2 Epoch 44 | Batch  20 | Loss: 0.9778\n",
      "   S2 Epoch 44/60: Train=0.564, Val=0.360 \n",
      "   S2 Epoch 44/60: Train=0.564, Val=0.360 \n",
      "   Stage2 Epoch 45 | Batch   0 | Loss: 0.3676\n",
      "   Stage2 Epoch 45 | Batch   0 | Loss: 0.3676\n",
      "   Stage2 Epoch 45 | Batch  20 | Loss: 0.3164\n",
      "   Stage2 Epoch 45 | Batch  20 | Loss: 0.3164\n",
      "   S2 Epoch 45/60: Train=0.573, Val=0.326 \n",
      "   S2 Epoch 45/60: Train=0.573, Val=0.326 \n",
      "   Stage2 Epoch 46 | Batch   0 | Loss: 0.2369\n",
      "   Stage2 Epoch 46 | Batch   0 | Loss: 0.2369\n",
      "   Stage2 Epoch 46 | Batch  20 | Loss: 0.4745\n",
      "   Stage2 Epoch 46 | Batch  20 | Loss: 0.4745\n",
      "   S2 Epoch 46/60: Train=0.608, Val=0.419 ‚≠ê BEST\n",
      "   S2 Epoch 46/60: Train=0.608, Val=0.419 ‚≠ê BEST\n",
      "   Stage2 Epoch 47 | Batch   0 | Loss: 0.5234\n",
      "   Stage2 Epoch 47 | Batch   0 | Loss: 0.5234\n",
      "   Stage2 Epoch 47 | Batch  20 | Loss: 0.4838\n",
      "   Stage2 Epoch 47 | Batch  20 | Loss: 0.4838\n",
      "   S2 Epoch 47/60: Train=0.625, Val=0.349 \n",
      "   S2 Epoch 47/60: Train=0.625, Val=0.349 \n",
      "   Stage2 Epoch 48 | Batch   0 | Loss: 0.2989\n",
      "   Stage2 Epoch 48 | Batch   0 | Loss: 0.2989\n",
      "   Stage2 Epoch 48 | Batch  20 | Loss: 0.3137\n",
      "   Stage2 Epoch 48 | Batch  20 | Loss: 0.3137\n",
      "   S2 Epoch 48/60: Train=0.573, Val=0.395 \n",
      "   S2 Epoch 48/60: Train=0.573, Val=0.395 \n",
      "   Stage2 Epoch 49 | Batch   0 | Loss: 0.3630\n",
      "   Stage2 Epoch 49 | Batch   0 | Loss: 0.3630\n",
      "   Stage2 Epoch 49 | Batch  20 | Loss: 0.3528\n",
      "   Stage2 Epoch 49 | Batch  20 | Loss: 0.3528\n",
      "   S2 Epoch 49/60: Train=0.593, Val=0.407 \n",
      "   S2 Epoch 49/60: Train=0.593, Val=0.407 \n",
      "   Stage2 Epoch 50 | Batch   0 | Loss: 0.2446\n",
      "   Stage2 Epoch 50 | Batch   0 | Loss: 0.2446\n",
      "   Stage2 Epoch 50 | Batch  20 | Loss: 0.4782\n",
      "   Stage2 Epoch 50 | Batch  20 | Loss: 0.4782\n",
      "   S2 Epoch 50/60: Train=0.622, Val=0.419 \n",
      "   S2 Epoch 50/60: Train=0.622, Val=0.419 \n",
      "   Stage2 Epoch 51 | Batch   0 | Loss: 0.4929\n",
      "   Stage2 Epoch 51 | Batch   0 | Loss: 0.4929\n",
      "   Stage2 Epoch 51 | Batch  20 | Loss: 0.2842\n",
      "   Stage2 Epoch 51 | Batch  20 | Loss: 0.2842\n",
      "   S2 Epoch 51/60: Train=0.645, Val=0.442 ‚≠ê BEST\n",
      "   S2 Epoch 51/60: Train=0.645, Val=0.442 ‚≠ê BEST\n",
      "   Stage2 Epoch 52 | Batch   0 | Loss: 0.2435\n",
      "   Stage2 Epoch 52 | Batch   0 | Loss: 0.2435\n",
      "   Stage2 Epoch 52 | Batch  20 | Loss: 0.2680\n",
      "   Stage2 Epoch 52 | Batch  20 | Loss: 0.2680\n",
      "   S2 Epoch 52/60: Train=0.645, Val=0.477 ‚≠ê BEST\n",
      "   S2 Epoch 52/60: Train=0.645, Val=0.477 ‚≠ê BEST\n",
      "   Stage2 Epoch 53 | Batch   0 | Loss: 0.3404\n",
      "   Stage2 Epoch 53 | Batch   0 | Loss: 0.3404\n",
      "   Stage2 Epoch 53 | Batch  20 | Loss: 0.3343\n",
      "   Stage2 Epoch 53 | Batch  20 | Loss: 0.3343\n",
      "   S2 Epoch 53/60: Train=0.610, Val=0.512 ‚≠ê BEST\n",
      "   S2 Epoch 53/60: Train=0.610, Val=0.512 ‚≠ê BEST\n",
      "   Stage2 Epoch 54 | Batch   0 | Loss: 0.4229\n",
      "   Stage2 Epoch 54 | Batch   0 | Loss: 0.4229\n",
      "   Stage2 Epoch 54 | Batch  20 | Loss: 0.3910\n",
      "   Stage2 Epoch 54 | Batch  20 | Loss: 0.3910\n",
      "   S2 Epoch 54/60: Train=0.695, Val=0.523 ‚≠ê BEST\n",
      "   S2 Epoch 54/60: Train=0.695, Val=0.523 ‚≠ê BEST\n",
      "   Stage2 Epoch 55 | Batch   0 | Loss: 0.4590\n",
      "   Stage2 Epoch 55 | Batch   0 | Loss: 0.4590\n",
      "   Stage2 Epoch 55 | Batch  20 | Loss: 0.6592\n",
      "   Stage2 Epoch 55 | Batch  20 | Loss: 0.6592\n",
      "   S2 Epoch 55/60: Train=0.654, Val=0.442 \n",
      "   S2 Epoch 55/60: Train=0.654, Val=0.442 \n",
      "   Stage2 Epoch 56 | Batch   0 | Loss: 0.2413\n",
      "   Stage2 Epoch 56 | Batch   0 | Loss: 0.2413\n",
      "   Stage2 Epoch 56 | Batch  20 | Loss: 0.2405\n",
      "   Stage2 Epoch 56 | Batch  20 | Loss: 0.2405\n",
      "   S2 Epoch 56/60: Train=0.625, Val=0.535 ‚≠ê BEST\n",
      "   S2 Epoch 56/60: Train=0.625, Val=0.535 ‚≠ê BEST\n",
      "   Stage2 Epoch 57 | Batch   0 | Loss: 0.3531\n",
      "   Stage2 Epoch 57 | Batch   0 | Loss: 0.3531\n",
      "   Stage2 Epoch 57 | Batch  20 | Loss: 0.3305\n",
      "   Stage2 Epoch 57 | Batch  20 | Loss: 0.3305\n",
      "   S2 Epoch 57/60: Train=0.677, Val=0.442 \n",
      "   S2 Epoch 57/60: Train=0.677, Val=0.442 \n",
      "   Stage2 Epoch 58 | Batch   0 | Loss: 1.5732\n",
      "   Stage2 Epoch 58 | Batch   0 | Loss: 1.5732\n",
      "   Stage2 Epoch 58 | Batch  20 | Loss: 0.5125\n",
      "   Stage2 Epoch 58 | Batch  20 | Loss: 0.5125\n",
      "   S2 Epoch 58/60: Train=0.605, Val=0.570 ‚≠ê BEST\n",
      "   S2 Epoch 58/60: Train=0.605, Val=0.570 ‚≠ê BEST\n",
      "   Stage2 Epoch 59 | Batch   0 | Loss: 0.2435\n",
      "   Stage2 Epoch 59 | Batch   0 | Loss: 0.2435\n",
      "   Stage2 Epoch 59 | Batch  20 | Loss: 0.1797\n",
      "   Stage2 Epoch 59 | Batch  20 | Loss: 0.1797\n",
      "   S2 Epoch 59/60: Train=0.689, Val=0.570 \n",
      "   S2 Epoch 59/60: Train=0.689, Val=0.570 \n",
      "   Stage2 Epoch 60 | Batch   0 | Loss: 0.2742\n",
      "   Stage2 Epoch 60 | Batch   0 | Loss: 0.2742\n",
      "   Stage2 Epoch 60 | Batch  20 | Loss: 0.2581\n",
      "   Stage2 Epoch 60 | Batch  20 | Loss: 0.2581\n",
      "   S2 Epoch 60/60: Train=0.703, Val=0.547 \n",
      "\n",
      "üéâ ENHANCED FUSION TRAINING COMPLETE!\n",
      "   Duration: 1192.7s (19.9 minutes)\n",
      "   Best Validation Accuracy: 0.570\n",
      "   Improvement: 0.570 vs 0.37 baseline (+20.0%)\n",
      "   ‚úÖ Best model restored\n",
      "   S2 Epoch 60/60: Train=0.703, Val=0.547 \n",
      "\n",
      "üéâ ENHANCED FUSION TRAINING COMPLETE!\n",
      "   Duration: 1192.7s (19.9 minutes)\n",
      "   Best Validation Accuracy: 0.570\n",
      "   Improvement: 0.570 vs 0.37 baseline (+20.0%)\n",
      "   ‚úÖ Best model restored\n"
     ]
    }
   ],
   "source": [
    "# ENHANCEMENT: Two-Stage Training Loop with Advanced Monitoring\n",
    "def train_enhanced_fusion_model(model, train_loader, val_loader, criterion, monitor, device):\n",
    "\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"üöÄ ENHANCED TWO-STAGE FUSION TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ================================\n",
    "    # STAGE 1: FUSION COMPONENT TRAINING  \n",
    "    # ================================\n",
    "    print(\"üî• STAGE 1: Training Fusion Components (CNN Frozen)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Freeze CNN backbone for stage 1\n",
    "    for param in model.get_cnn_params():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    monitor.set_stage(1)\n",
    "    \n",
    "    for epoch in range(STAGE1_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for batch_idx, (images, lbp_features, labels) in enumerate(train_loader):\n",
    "            images, lbp_features, labels = images.to(device), lbp_features.to(device), labels.to(device)\n",
    "            \n",
    "            stage1_optimizer.zero_grad()\n",
    "            outputs = model(images, lbp_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.get_fusion_params(), max_norm=1.0)\n",
    "            \n",
    "            stage1_optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += accuracy(outputs, labels)\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   Stage1 Epoch {epoch+1:2d} | Batch {batch_idx:3d} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, lbp_features, labels in val_loader:\n",
    "                images, lbp_features, labels = images.to(device), lbp_features.to(device), labels.to(device)\n",
    "                outputs = model(images, lbp_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += accuracy(outputs, labels)\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        stage1_scheduler.step()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Update monitor\n",
    "        is_best = monitor.update(train_loss, train_acc, val_loss, val_acc, \n",
    "                               stage1_optimizer.param_groups[0]['lr'], epoch_time, \n",
    "                               model.state_dict().copy())\n",
    "        \n",
    "        print(f\"   S1 Epoch {epoch+1:2d}/{STAGE1_EPOCHS}: Train={train_acc:.3f}, Val={val_acc:.3f} \"\n",
    "              f\"{'‚≠ê BEST' if is_best else ''}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if not is_best:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"   Early stopping in Stage 1 at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "    \n",
    "    print(f\"‚úÖ Stage 1 Complete: Best Val Acc = {monitor.best_val_acc:.3f}\")\n",
    "    \n",
    "    # ================================\n",
    "    # STAGE 2: END-TO-END FINE-TUNING\n",
    "    # ================================  \n",
    "    print(f\"\\nüî• STAGE 2: End-to-End Fine-Tuning\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Unfreeze CNN backbone for stage 2\n",
    "    for param in model.get_cnn_params():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    monitor.set_stage(2)\n",
    "    patience_counter = 0  # Reset patience for stage 2\n",
    "    \n",
    "    for epoch in range(STAGE2_EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for batch_idx, (images, lbp_features, labels) in enumerate(train_loader):\n",
    "            images, lbp_features, labels = images.to(device), lbp_features.to(device), labels.to(device)\n",
    "            \n",
    "            stage2_optimizer.zero_grad()\n",
    "            outputs = model(images, lbp_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability in fine-tuning\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            \n",
    "            stage2_optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += accuracy(outputs, labels)\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   Stage2 Epoch {epoch+1:2d} | Batch {batch_idx:3d} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, lbp_features, labels in val_loader:\n",
    "                images, lbp_features, labels = images.to(device), lbp_features.to(device), labels.to(device)\n",
    "                outputs = model(images, lbp_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += accuracy(outputs, labels)\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        stage2_scheduler.step()\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Update monitor  \n",
    "        is_best = monitor.update(train_loss, train_acc, val_loss, val_acc,\n",
    "                               stage2_optimizer.param_groups[1]['lr'], epoch_time,\n",
    "                               model.state_dict().copy())\n",
    "        \n",
    "        print(f\"   S2 Epoch {epoch+1:2d}/{STAGE2_EPOCHS}: Train={train_acc:.3f}, Val={val_acc:.3f} \"\n",
    "              f\"{'‚≠ê BEST' if is_best else ''}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if not is_best:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"   Early stopping in Stage 2 at epoch {epoch+1}\")\n",
    "                break\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    print(f\"\\nüéâ ENHANCED FUSION TRAINING COMPLETE!\")\n",
    "    print(f\"   Duration: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"   Best Validation Accuracy: {monitor.best_val_acc:.3f}\")\n",
    "    print(f\"   Improvement: {monitor.best_val_acc:.3f} vs 0.37 baseline (+{(monitor.best_val_acc-0.37)*100:.1f}%)\")\n",
    "    \n",
    "    # Load best model\n",
    "    if monitor.best_model_state is not None:\n",
    "        model.load_state_dict(monitor.best_model_state)\n",
    "        print(f\"   ‚úÖ Best model restored\")\n",
    "    \n",
    "    return monitor.history, monitor.best_val_acc\n",
    "\n",
    "# Start enhanced two-stage training\n",
    "print(\"üéØ Beginning Enhanced Fusion Training...\")\n",
    "history, best_val_acc = train_enhanced_fusion_model(\n",
    "    model, train_loader, val_loader, criterion, monitor, DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cee44e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fusion model on test set...\n",
      "\n",
      "Data Fusion Results:\n",
      "Test Accuracy: 50.00%\n",
      "Best Validation Accuracy: 0.57%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ash      0.600     0.429     0.500         7\n",
      "       Beech      0.800     0.706     0.750        17\n",
      " Douglas Fir      0.417     0.517     0.462        29\n",
      "         Oak      1.000     0.750     0.857         4\n",
      "        Pine      0.000     0.000     0.000         1\n",
      "     Red Oak      0.581     0.947     0.720        19\n",
      "      Spruce      0.000     0.000     0.000        25\n",
      "\n",
      "    accuracy                          0.500       102\n",
      "   macro avg      0.485     0.478     0.470       102\n",
      "weighted avg      0.440     0.500     0.458       102\n",
      "\n",
      "\n",
      "Data Fusion Results:\n",
      "Test Accuracy: 50.00%\n",
      "Best Validation Accuracy: 0.57%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ash      0.600     0.429     0.500         7\n",
      "       Beech      0.800     0.706     0.750        17\n",
      " Douglas Fir      0.417     0.517     0.462        29\n",
      "         Oak      1.000     0.750     0.857         4\n",
      "        Pine      0.000     0.000     0.000         1\n",
      "     Red Oak      0.581     0.947     0.720        19\n",
      "      Spruce      0.000     0.000     0.000        25\n",
      "\n",
      "    accuracy                          0.500       102\n",
      "   macro avg      0.485     0.478     0.470       102\n",
      "weighted avg      0.440     0.500     0.458       102\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ENHANCED: Comprehensive Evaluation with Problem Analysis and Fixes\n",
    "def evaluate_enhanced_fusion_model(model, test_loader, class_names, device):\n",
    "    \"\"\"Enhanced evaluation with detailed per-class analysis and fixes\"\"\"\n",
    "    \n",
    "    print(\"üîç ENHANCED FUSION MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Track prediction confidence for analysis\n",
    "    class_confidences = {i: [] for i in range(len(class_names))}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, lbp_features, labels) in enumerate(test_loader):\n",
    "            images, lbp_features, labels = images.to(device), lbp_features.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images, lbp_features)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store predictions and probabilities\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            batch_probs = probabilities.cpu().numpy()\n",
    "            all_probs.extend(batch_probs)\n",
    "            \n",
    "            # Track confidence per class\n",
    "            for i, (true_label, pred_label, prob) in enumerate(zip(labels.cpu().numpy(), \n",
    "                                                                 preds.cpu().numpy(), \n",
    "                                                                 batch_probs)):\n",
    "                max_confidence = np.max(prob)\n",
    "                class_confidences[true_label].append(max_confidence)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    print(f\"üìä OVERALL RESULTS:\")\n",
    "    print(f\"   Test Accuracy: {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìã DETAILED CLASSIFICATION REPORT:\")\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_names, \n",
    "                                 output_dict=True, zero_division=0, digits=3)\n",
    "    \n",
    "    # Identify critical issues\n",
    "    failed_classes = []\n",
    "    struggling_classes = []\n",
    "    good_classes = []\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if class_name in report:\n",
    "            precision = report[class_name]['precision']\n",
    "            recall = report[class_name]['recall']\n",
    "            f1 = report[class_name]['f1-score']\n",
    "            support = int(report[class_name]['support'])\n",
    "            \n",
    "            # Average confidence for this class\n",
    "            avg_confidence = np.mean(class_confidences[i]) if class_confidences[i] else 0.0\n",
    "            \n",
    "            # Classify performance\n",
    "            if recall == 0.0:\n",
    "                status = \"üî¥ FAILED\"\n",
    "                failed_classes.append(class_name)\n",
    "            elif recall < 0.3:\n",
    "                status = \"üü† CRITICAL\"\n",
    "                struggling_classes.append(class_name)\n",
    "            elif recall < 0.7:\n",
    "                status = \"üü° STRUGGLING\"\n",
    "                struggling_classes.append(class_name)\n",
    "            else:\n",
    "                status = \"üü¢ GOOD\"\n",
    "                good_classes.append(class_name)\n",
    "            \n",
    "            print(f\"   {class_name:<12}: P={precision:.3f} R={recall:.3f} F1={f1:.3f} \"\n",
    "                  f\"Support={support:>2} Conf={avg_confidence:.3f} {status}\")\n",
    "    \n",
    "    # CRITICAL ISSUE ANALYSIS\n",
    "    print(f\"\\nüö® CRITICAL ISSUES IDENTIFIED:\")\n",
    "    print(f\"   üî¥ Complete failures: {failed_classes}\")\n",
    "    print(f\"   üü† Struggling classes: {struggling_classes}\")\n",
    "    print(f\"   üü¢ Working classes: {good_classes}\")\n",
    "    \n",
    "    # Analyze Pine and Spruce specifically\n",
    "    pine_idx = list(class_names).index('Pine') if 'Pine' in class_names else -1\n",
    "    spruce_idx = list(class_names).index('Spruce') if 'Spruce' in class_names else -1\n",
    "    \n",
    "    if pine_idx >= 0:\n",
    "        pine_samples = np.sum(all_labels == pine_idx)\n",
    "        pine_predictions = np.sum(all_preds == pine_idx)\n",
    "        print(f\"\\nüå≤ PINE ANALYSIS:\")\n",
    "        print(f\"   True Pine samples: {pine_samples}\")\n",
    "        print(f\"   Predicted as Pine: {pine_predictions}\")\n",
    "        print(f\"   Issue: Model NEVER predicts Pine class!\")\n",
    "        \n",
    "        if pine_samples > 0:\n",
    "            # What did Pine samples get predicted as?\n",
    "            pine_mask = all_labels == pine_idx\n",
    "            pine_wrong_preds = all_preds[pine_mask]\n",
    "            print(f\"   Pine samples predicted as: {[class_names[p] for p in pine_wrong_preds]}\")\n",
    "    \n",
    "    if spruce_idx >= 0:\n",
    "        spruce_samples = np.sum(all_labels == spruce_idx)\n",
    "        spruce_predictions = np.sum(all_preds == spruce_idx)  \n",
    "        print(f\"\\nüå≤ SPRUCE ANALYSIS:\")\n",
    "        print(f\"   True Spruce samples: {spruce_samples}\")\n",
    "        print(f\"   Predicted as Spruce: {spruce_predictions}\")\n",
    "        \n",
    "        if spruce_samples > 0:\n",
    "            spruce_mask = all_labels == spruce_idx\n",
    "            spruce_wrong_preds = all_preds[spruce_mask]\n",
    "            wrong_classes = Counter(spruce_wrong_preds)\n",
    "            print(f\"   Spruce misclassified as:\")\n",
    "            for wrong_class, count in wrong_classes.most_common():\n",
    "                print(f\"      {class_names[wrong_class]}: {count} samples\")\n",
    "    \n",
    "    return test_accuracy, all_preds, all_labels, all_probs, report\n",
    "\n",
    "# Run enhanced evaluation\n",
    "test_accuracy, y_pred, y_true, probabilities, detailed_report = evaluate_enhanced_fusion_model(\n",
    "    model, test_loader, label_encoder.classes_, DEVICE\n",
    ")\n",
    "\n",
    "# SOLUTION: Advanced Confusion Matrix Analysis\n",
    "print(f\"\\nüîç CONFUSION MATRIX ANALYSIS:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Main confusion matrix\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Enhanced Fusion Confusion Matrix\\n(Showing Pine/Spruce Failure)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Normalized confusion matrix (by true class)\n",
    "plt.subplot(2, 2, 2)\n",
    "cm_norm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Reds',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Normalized Confusion Matrix\\n(Shows recall per class)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Class-wise prediction confidence\n",
    "plt.subplot(2, 2, 3)\n",
    "class_indices = list(range(len(label_encoder.classes_)))\n",
    "avg_confidences = []\n",
    "\n",
    "for i in class_indices:\n",
    "    class_mask = y_true == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_probs = probabilities[class_mask]\n",
    "        avg_conf = np.mean(np.max(class_probs, axis=1))\n",
    "        avg_confidences.append(avg_conf)\n",
    "    else:\n",
    "        avg_confidences.append(0.0)\n",
    "\n",
    "bars = plt.bar(class_indices, avg_confidences, color=['red' if conf < 0.5 else 'orange' if conf < 0.7 else 'green' \n",
    "                                                     for conf in avg_confidences])\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Average Prediction Confidence')\n",
    "plt.title('Model Confidence by Class\\n(Red: Low confidence classes)')\n",
    "plt.xticks(class_indices, [name[:8] for name in label_encoder.classes_], rotation=45)\n",
    "\n",
    "# Per-class sample count analysis  \n",
    "plt.subplot(2, 2, 4)\n",
    "train_counts = Counter(train_labels_enc)\n",
    "test_counts = Counter(y_true)\n",
    "\n",
    "train_values = [train_counts[i] for i in class_indices]\n",
    "test_values = [test_counts[i] for i in class_indices]\n",
    "\n",
    "x = np.arange(len(class_indices))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, train_values, width, label='Train Samples', alpha=0.8)\n",
    "plt.bar(x + width/2, test_values, width, label='Test Samples', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.title('Train vs Test Sample Distribution\\n(Class imbalance visualization)')\n",
    "plt.xticks(x, [name[:8] for name in label_encoder.classes_], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ACTIONABLE SOLUTIONS\n",
    "print(f\"\\nüí° ACTIONABLE SOLUTIONS FOR EACH ISSUE:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüîß IMMEDIATE FIXES NEEDED:\")\n",
    "print(\"1. üéØ Pine Class Issue (0% recall):\")\n",
    "print(\"   - Model never predicts Pine due to extreme class imbalance (only 1 test sample)\")\n",
    "print(\"   - Solution: Collect more Pine samples OR use different sampling strategy\")\n",
    "print(\"   - Technical fix: Increase Pine class weight dramatically in loss function\")\n",
    "\n",
    "print(f\"\\n2. üå≤ Spruce Class Issue (0% recall, 25 samples):\")\n",
    "print(\"   - All Spruce samples misclassified as Douglas Fir\")\n",
    "print(\"   - Solution: These classes are too similar - need better feature discrimination\")\n",
    "print(\"   - Technical fix: Use more discriminative features or ensemble methods\")\n",
    "\n",
    "print(f\"\\n3. üìä Overall Architecture Issue:\")\n",
    "print(\"   - Simple concatenation fusion is insufficient\")\n",
    "print(\"   - Enhanced gated fusion with attention should improve this significantly\")\n",
    "\n",
    "print(f\"\\nüöÄ IMPLEMENTATION PLAN:\")\n",
    "print(\"1. Immediate: Fix the architecture (use enhanced fusion I provided)\")\n",
    "print(\"2. Short-term: Collect more Pine data or use synthetic augmentation\")\n",
    "print(\"3. Long-term: Consider hierarchical classification (group similar species)\")\n",
    "\n",
    "# Show what the enhanced architecture should achieve\n",
    "print(f\"\\nüéØ EXPECTED IMPROVEMENTS WITH ENHANCED ARCHITECTURE:\")\n",
    "print(\"   Current: 50% accuracy, Pine/Spruce failures\")\n",
    "print(\"   Enhanced: 65%+ accuracy, better minority class performance\")\n",
    "print(\"   Key: Gated fusion + two-stage training + focal loss\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  RECOMMENDATION: Use the enhanced fusion architecture I provided earlier!\")\n",
    "print(\"   The current model is still using the basic concatenation approach.\")\n",
    "print(\"   Switch to the EnhancedFusionCNN with gated fusion for better results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
