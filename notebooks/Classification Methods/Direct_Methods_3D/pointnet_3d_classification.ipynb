{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42105f5a",
   "metadata": {},
   "source": [
    "# 🌳 PointNet for 3D Tree Species Classification\n",
    "\n",
    "## Method: Direct 3D Point Cloud Classification\n",
    "This notebook implements the **PointNet** deep learning model for direct 3D point cloud classification.\n",
    "\n",
    "## Key Features:\n",
    "1. **Direct 3D processing**: Works directly with point cloud data\n",
    "2. **Farthest Point Sampling (FPS)**: Samples exactly 1024 points from variable-sized clouds\n",
    "3. **Data normalization**: Centers and scales points to unit sphere\n",
    "4. **PointNet architecture**: Complete implementation with T-Net and classification head\n",
    "5. **7-class classification**: Adapted for tree species classification\n",
    "\n",
    "## Pipeline:\n",
    "1. **Data loading**: Load `.xyz`, `.pts` files from train/test folders\n",
    "2. **Preprocessing**: FPS sampling + normalization\n",
    "3. **Model training**: PointNet with T-Net transformation\n",
    "4. **Evaluation**: Classification report and confusion matrix\n",
    "\n",
    "Expected: Good performance on geometric features of tree species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9a5a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌳 PointNet 3D Tree Species Classification\n",
      "==================================================\n",
      "   Device: mps\n",
      "   Points per cloud: 1024\n",
      "   Batch size: 32\n",
      "   Learning rate: 0.001\n",
      "   Method: Direct 3D point cloud processing\n"
     ]
    }
   ],
   "source": [
    "# PointNet 3D Tree Classification - Complete Pipeline\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PointNet Configuration\n",
    "NUM_POINTS = 1024        # Fixed number of points after FPS\n",
    "BATCH_SIZE = 32          # Batch size for training\n",
    "LEARNING_RATE = 1e-3     # Learning rate\n",
    "NUM_EPOCHS = 100         # Training epochs\n",
    "PATIENCE = 20            # Early stopping patience\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"🌳 PointNet 3D Tree Species Classification\")\n",
    "print(\"=\"*50)\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Points per cloud: {NUM_POINTS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Method: Direct 3D point cloud processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bce9967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PointCloudDataset class defined with FPS and normalization\n"
     ]
    }
   ],
   "source": [
    "# PointCloud Dataset Class with FPS and Normalization\n",
    "class PointCloudDataset(Dataset):\n",
    "    \"\"\"Dataset for 3D point clouds with FPS sampling and normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, num_points=1024, split='train'):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.num_points = num_points\n",
    "        self.split = split\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        self.file_paths, self.labels = self._load_data()\n",
    "        print(f\"   Loaded {len(self.file_paths)} {split} files\")\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Load all point cloud file paths and labels\"\"\"\n",
    "        file_paths, labels = [], []\n",
    "        \n",
    "        for species_dir in self.data_path.iterdir():\n",
    "            if species_dir.is_dir():\n",
    "                # Get all point cloud files\n",
    "                files = list(species_dir.glob(\"*.xyz\")) + list(species_dir.glob(\"*.pts\")) + list(species_dir.glob(\"*.txt\"))\n",
    "                print(f\"   {species_dir.name}: {len(files)} files\")\n",
    "                \n",
    "                file_paths.extend(files)\n",
    "                labels.extend([species_dir.name] * len(files))\n",
    "        \n",
    "        return file_paths, labels\n",
    "    \n",
    "    def _load_point_cloud(self, file_path):\n",
    "        \"\"\"Load point cloud from file\"\"\"\n",
    "        try:\n",
    "            # Try different loading methods based on file extension\n",
    "            if file_path.suffix.lower() in ['.xyz', '.txt']:\n",
    "                # Load as text file\n",
    "                points = np.loadtxt(file_path, usecols=(0, 1, 2))\n",
    "            elif file_path.suffix.lower() == '.pts':\n",
    "                # Load .pts file (sometimes has header)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                # Skip header if present\n",
    "                start_idx = 0\n",
    "                for i, line in enumerate(lines):\n",
    "                    try:\n",
    "                        float(line.split()[0])\n",
    "                        start_idx = i\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                points = []\n",
    "                for line in lines[start_idx:]:\n",
    "                    coords = line.strip().split()\n",
    "                    if len(coords) >= 3:\n",
    "                        points.append([float(coords[0]), float(coords[1]), float(coords[2])])\n",
    "                \n",
    "                points = np.array(points)\n",
    "            else:\n",
    "                # Fallback: try Open3D\n",
    "                pcd = o3d.io.read_point_cloud(str(file_path))\n",
    "                points = np.asarray(pcd.points)\n",
    "            \n",
    "            return points\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load {file_path}: {e}\")\n",
    "            # Return a dummy point cloud\n",
    "            return np.random.randn(100, 3)\n",
    "    \n",
    "    def _farthest_point_sampling(self, points, num_samples):\n",
    "        \"\"\"Perform Farthest Point Sampling using Open3D\"\"\"\n",
    "        if len(points) <= num_samples:\n",
    "            # If we have fewer points, pad with random noise\n",
    "            padding = np.random.randn(num_samples - len(points), 3) * 0.01\n",
    "            points = np.vstack([points, padding])\n",
    "            return points\n",
    "        \n",
    "        try:\n",
    "            # Create Open3D point cloud\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(points)\n",
    "            \n",
    "            # Perform FPS\n",
    "            sampled_pcd = pcd.farthest_point_down_sample(num_samples)\n",
    "            return np.asarray(sampled_pcd.points)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: FPS failed, using random sampling: {e}\")\n",
    "            # Fallback to random sampling\n",
    "            indices = np.random.choice(len(points), num_samples, replace=False)\n",
    "            return points[indices]\n",
    "    \n",
    "    def _normalize_point_cloud(self, points):\n",
    "        \"\"\"Normalize point cloud to unit sphere\"\"\"\n",
    "        # Center the point cloud\n",
    "        centroid = np.mean(points, axis=0)\n",
    "        points = points - centroid\n",
    "        \n",
    "        # Scale to unit sphere\n",
    "        max_dist = np.max(np.sqrt(np.sum(points**2, axis=1)))\n",
    "        if max_dist > 0:\n",
    "            points = points / max_dist\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load point cloud\n",
    "        points = self._load_point_cloud(self.file_paths[idx])\n",
    "        \n",
    "        # Apply FPS to get exactly num_points\n",
    "        points = self._farthest_point_sampling(points, self.num_points)\n",
    "        \n",
    "        # Normalize to unit sphere\n",
    "        points = self._normalize_point_cloud(points)\n",
    "        \n",
    "        # Convert to tensor (N, 3) -> (3, N) for PointNet\n",
    "        points = torch.FloatTensor(points).transpose(0, 1)\n",
    "        \n",
    "        return points, self.labels[idx]\n",
    "\n",
    "print(\"✅ PointCloudDataset class defined with FPS and normalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7053d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ T-Net transformation network defined\n"
     ]
    }
   ],
   "source": [
    "# T-Net (Transformation Network) for PointNet\n",
    "class TNet(nn.Module):\n",
    "    \"\"\"T-Net transformation network for spatial transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, k=3):\n",
    "        super(TNet, self).__init__()\n",
    "        self.k = k\n",
    "        \n",
    "        # Shared MLPs\n",
    "        self.conv1 = nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Initialize transformation matrix as identity\n",
    "        self.register_buffer('identity', torch.eye(k).flatten())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Shared MLPs\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # Global max pooling\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Add identity transformation\n",
    "        x = x + self.identity.expand(batch_size, -1)\n",
    "        \n",
    "        # Reshape to transformation matrix\n",
    "        x = x.view(batch_size, self.k, self.k)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"✅ T-Net transformation network defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df4fa1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PointNet classification model defined\n"
     ]
    }
   ],
   "source": [
    "# PointNet Classification Model\n",
    "class PointNet(nn.Module):\n",
    "    \"\"\"PointNet model for 3D point cloud classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=7, num_points=1024):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_points = num_points\n",
    "        \n",
    "        # Input transformation (T-Net)\n",
    "        self.input_transform = TNet(k=3)\n",
    "        \n",
    "        # Shared MLPs for point-wise features\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 64, 1)\n",
    "        \n",
    "        # Feature transformation (T-Net)\n",
    "        self.feature_transform = TNet(k=64)\n",
    "        \n",
    "        # More shared MLPs\n",
    "        self.conv3 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv4 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv5 = nn.Conv1d(128, 1024, 1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(1024)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        print(f\"🏗️ PointNet Model:\")\n",
    "        print(f\"   Input: {num_points} points × 3 coordinates\")\n",
    "        print(f\"   Features: T-Net + Shared MLPs + Global Max Pool\")\n",
    "        print(f\"   Output: {num_classes} tree species classes\")\n",
    "        print(f\"   Architecture: Input T-Net + Feature T-Net + Classifier\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_points = x.size(2)\n",
    "        \n",
    "        # Input transformation\n",
    "        input_trans = self.input_transform(x)  # (B, 3, 3)\n",
    "        x = torch.bmm(input_trans, x)  # Apply transformation\n",
    "        \n",
    "        # First set of shared MLPs\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        # Feature transformation\n",
    "        feature_trans = self.feature_transform(x)  # (B, 64, 64)\n",
    "        x = torch.bmm(feature_trans, x)  # Apply feature transformation\n",
    "        \n",
    "        # Second set of shared MLPs\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        \n",
    "        # Global max pooling to get shape signature\n",
    "        x = torch.max(x, 2, keepdim=True)[0]  # (B, 1024, 1)\n",
    "        x = x.view(batch_size, -1)  # (B, 1024)\n",
    "        \n",
    "        # Classification head\n",
    "        x = F.relu(self.bn6(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn7(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"✅ PointNet classification model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "720f93cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading 3D point cloud datasets...\n",
      "   Oak: 18 files\n",
      "   Douglas Fir: 147 files\n",
      "   Spruce: 127 files\n",
      "   Pine: 20 files\n",
      "   Ash: 32 files\n",
      "   Red Oak: 81 files\n",
      "   Beech: 132 files\n",
      "   Loaded 557 train files\n",
      "   Oak: 4 files\n",
      "   Douglas Fir: 36 files\n",
      "   Spruce: 31 files\n",
      "   Pine: 5 files\n",
      "   Ash: 7 files\n",
      "   Red Oak: 19 files\n",
      "   Beech: 32 files\n",
      "   Loaded 134 test files\n",
      "\n",
      "📊 Dataset Information:\n",
      "   Classes: 7 tree species\n",
      "   Class names: [np.str_('Ash'), np.str_('Beech'), np.str_('Douglas Fir'), np.str_('Oak'), np.str_('Pine'), np.str_('Red Oak'), np.str_('Spruce')]\n",
      "   Train samples: 557\n",
      "   Test samples: 134\n",
      "\n",
      "✅ Data loaders created:\n",
      "   Train batches: 14\n",
      "   Validation batches: 4\n",
      "   Test batches: 5\n",
      "\n",
      "🔍 Testing data loading...\n",
      "   Batch 1: Points shape: torch.Size([32, 3, 1024]), Labels shape: torch.Size([32])\n",
      "   Points range: [-1.000, 0.829]\n",
      "✅ Data loading successful!\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing\n",
    "print(\"📁 Loading 3D point cloud datasets...\")\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = PointCloudDataset(\"../../../train\", num_points=NUM_POINTS, split='train')\n",
    "test_dataset = PointCloudDataset(\"../../../test\", num_points=NUM_POINTS, split='test')\n",
    "\n",
    "# Get class names and encode labels\n",
    "all_labels = train_dataset.labels + test_dataset.labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\n📊 Dataset Information:\")\n",
    "print(f\"   Classes: {num_classes} tree species\")\n",
    "print(f\"   Class names: {list(class_names)}\")\n",
    "print(f\"   Train samples: {len(train_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Encode labels for datasets\n",
    "train_dataset.labels = label_encoder.transform(train_dataset.labels)\n",
    "test_dataset.labels = label_encoder.transform(test_dataset.labels)\n",
    "\n",
    "# Create validation split from training data\n",
    "train_indices = list(range(len(train_dataset)))\n",
    "train_idx, val_idx = train_test_split(\n",
    "    train_indices, test_size=0.2, random_state=42, \n",
    "    stratify=train_dataset.labels\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    torch.utils.data.Subset(train_dataset, train_idx),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    torch.utils.data.Subset(train_dataset, val_idx),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Data loaders created:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading\n",
    "print(f\"\\n🔍 Testing data loading...\")\n",
    "for batch_idx, (points, labels) in enumerate(train_loader):\n",
    "    print(f\"   Batch {batch_idx + 1}: Points shape: {points.shape}, Labels shape: {labels.shape}\")\n",
    "    print(f\"   Points range: [{points.min():.3f}, {points.max():.3f}]\")\n",
    "    if batch_idx == 0:  # Just test first batch\n",
    "        break\n",
    "\n",
    "print(\"✅ Data loading successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76a983bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Initializing PointNet model...\n",
      "🏗️ PointNet Model:\n",
      "   Input: 1024 points × 3 coordinates\n",
      "   Features: T-Net + Shared MLPs + Global Max Pool\n",
      "   Output: 7 tree species classes\n",
      "   Architecture: Input T-Net + Feature T-Net + Classifier\n",
      "   Total parameters: 3,471,568\n",
      "   Trainable parameters: 3,471,568\n",
      "\n",
      "🔧 Training Setup:\n",
      "   Loss: CrossEntropyLoss\n",
      "   Optimizer: Adam @ 0.001\n",
      "   Scheduler: ReduceLROnPlateau\n",
      "   Device: mps\n",
      "   Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization and Training Setup\n",
    "print(\"🏗️ Initializing PointNet model...\")\n",
    "\n",
    "# Create model\n",
    "model = PointNet(num_classes=num_classes, num_points=NUM_POINTS).to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "def accuracy_metric(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    return torch.sum(preds == labels).item()\n",
    "\n",
    "print(f\"\\n🔧 Training Setup:\")\n",
    "print(f\"   Loss: CrossEntropyLoss\")\n",
    "print(f\"   Optimizer: Adam @ {LEARNING_RATE}\")\n",
    "print(f\"   Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d41be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌳 Starting PointNet Training...\n",
      "🚀 POINTNET TRAINING (3D Point Clouds)\n",
      "==================================================\n",
      "   Epoch  1 | Batch  0/14 | Loss: 1.9338 | Acc: 0.281\n",
      "   Epoch  1 | Batch 10/14 | Loss: 1.6840 | Acc: 0.375\n",
      "   Epoch  1: Train=0.326, Val=0.223, Loss=1.7338, Time=141.5s\n",
      "      ⭐ NEW BEST: 0.223\n",
      "--------------------------------------------------\n",
      "   Epoch  2 | Batch  0/14 | Loss: 1.3696 | Acc: 0.375\n",
      "   Epoch  2 | Batch 10/14 | Loss: 1.5162 | Acc: 0.375\n",
      "   Epoch  2: Train=0.438, Val=0.062, Loss=1.3658, Time=156.6s\n",
      "--------------------------------------------------\n",
      "   Epoch  3 | Batch  0/14 | Loss: 1.1403 | Acc: 0.438\n",
      "   Epoch  3 | Batch 10/14 | Loss: 1.2095 | Acc: 0.531\n",
      "   Epoch  3: Train=0.488, Val=0.161, Loss=1.2119, Time=153.4s\n",
      "--------------------------------------------------\n",
      "   Epoch  4 | Batch  0/14 | Loss: 0.9586 | Acc: 0.531\n",
      "   Epoch  4 | Batch 10/14 | Loss: 1.0907 | Acc: 0.531\n",
      "   Epoch  4: Train=0.458, Val=0.464, Loss=1.1761, Time=154.7s\n",
      "      ⭐ NEW BEST: 0.464\n",
      "--------------------------------------------------\n",
      "   Epoch  5 | Batch  0/14 | Loss: 1.0589 | Acc: 0.344\n",
      "   Epoch  5 | Batch 10/14 | Loss: 1.2073 | Acc: 0.344\n",
      "   Epoch  5: Train=0.503, Val=0.464, Loss=1.1225, Time=155.0s\n",
      "--------------------------------------------------\n",
      "   Epoch  6 | Batch  0/14 | Loss: 1.0682 | Acc: 0.438\n",
      "   Epoch  6 | Batch 10/14 | Loss: 0.8042 | Acc: 0.688\n",
      "   Epoch  6: Train=0.515, Val=0.482, Loss=1.0762, Time=153.2s\n",
      "      ⭐ NEW BEST: 0.482\n",
      "--------------------------------------------------\n",
      "   Epoch  7 | Batch  0/14 | Loss: 1.0743 | Acc: 0.469\n",
      "   Epoch  7 | Batch 10/14 | Loss: 0.7586 | Acc: 0.625\n",
      "   Epoch  7: Train=0.551, Val=0.446, Loss=1.0189, Time=5336.4s\n",
      "--------------------------------------------------\n",
      "   Epoch  8 | Batch  0/14 | Loss: 0.8140 | Acc: 0.531\n",
      "   Epoch  8 | Batch 10/14 | Loss: 0.8309 | Acc: 0.656\n",
      "   Epoch  8: Train=0.600, Val=0.348, Loss=0.9593, Time=153.2s\n",
      "--------------------------------------------------\n",
      "   Epoch  9 | Batch  0/14 | Loss: 0.7728 | Acc: 0.688\n",
      "   Epoch  9 | Batch 10/14 | Loss: 0.9771 | Acc: 0.625\n",
      "   Epoch  9: Train=0.584, Val=0.509, Loss=1.0102, Time=134.0s\n",
      "      ⭐ NEW BEST: 0.509\n",
      "--------------------------------------------------\n",
      "   Epoch 10 | Batch  0/14 | Loss: 1.0312 | Acc: 0.625\n",
      "   Epoch 10 | Batch 10/14 | Loss: 0.8577 | Acc: 0.625\n",
      "   Epoch 10: Train=0.658, Val=0.518, Loss=0.8734, Time=131.3s\n",
      "      ⭐ NEW BEST: 0.518\n",
      "--------------------------------------------------\n",
      "   Epoch 11 | Batch  0/14 | Loss: 0.6393 | Acc: 0.812\n",
      "   Epoch 11 | Batch 10/14 | Loss: 0.8619 | Acc: 0.625\n",
      "   Epoch 11: Train=0.692, Val=0.589, Loss=0.8495, Time=135.3s\n",
      "      ⭐ NEW BEST: 0.589\n",
      "--------------------------------------------------\n",
      "   Epoch 12 | Batch  0/14 | Loss: 0.6192 | Acc: 0.750\n",
      "   Epoch 12 | Batch 10/14 | Loss: 1.4993 | Acc: 0.438\n",
      "   Epoch 12: Train=0.681, Val=0.536, Loss=0.7993, Time=133.6s\n",
      "--------------------------------------------------\n",
      "   Epoch 13 | Batch  0/14 | Loss: 0.6221 | Acc: 0.750\n",
      "   Epoch 13 | Batch 10/14 | Loss: 0.7666 | Acc: 0.719\n",
      "   Epoch 13: Train=0.721, Val=0.420, Loss=0.7783, Time=132.0s\n",
      "--------------------------------------------------\n",
      "   Epoch 14 | Batch  0/14 | Loss: 0.7268 | Acc: 0.656\n",
      "   Epoch 14 | Batch 10/14 | Loss: 0.5939 | Acc: 0.812\n",
      "   Epoch 14: Train=0.766, Val=0.473, Loss=0.6388, Time=147.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 15 | Batch  0/14 | Loss: 0.6449 | Acc: 0.781\n",
      "   Epoch 15 | Batch 10/14 | Loss: 0.6296 | Acc: 0.781\n",
      "   Epoch 15: Train=0.753, Val=0.464, Loss=0.6974, Time=153.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 16 | Batch  0/14 | Loss: 0.4279 | Acc: 0.844\n",
      "   Epoch 16 | Batch 10/14 | Loss: 0.9680 | Acc: 0.656\n",
      "   Epoch 16: Train=0.712, Val=0.491, Loss=0.7413, Time=148.4s\n",
      "--------------------------------------------------\n",
      "   Epoch 17 | Batch  0/14 | Loss: 0.5713 | Acc: 0.781\n",
      "   Epoch 17 | Batch 10/14 | Loss: 0.8904 | Acc: 0.656\n",
      "   Epoch 17: Train=0.751, Val=0.545, Loss=0.6869, Time=131.7s\n",
      "--------------------------------------------------\n",
      "   Epoch 18 | Batch  0/14 | Loss: 0.3749 | Acc: 0.844\n",
      "   Epoch 18 | Batch 10/14 | Loss: 0.4736 | Acc: 0.781\n",
      "   Epoch 18: Train=0.796, Val=0.509, Loss=0.5445, Time=130.9s\n",
      "--------------------------------------------------\n",
      "   Epoch 19 | Batch  0/14 | Loss: 0.6737 | Acc: 0.750\n",
      "   Epoch 19 | Batch 10/14 | Loss: 1.3101 | Acc: 0.656\n",
      "   Epoch 19: Train=0.748, Val=0.464, Loss=0.7194, Time=131.6s\n",
      "--------------------------------------------------\n",
      "   Epoch 20 | Batch  0/14 | Loss: 0.5581 | Acc: 0.812\n",
      "   Epoch 20 | Batch 10/14 | Loss: 0.4720 | Acc: 0.844\n",
      "   Epoch 20: Train=0.811, Val=0.482, Loss=0.5210, Time=133.4s\n",
      "--------------------------------------------------\n",
      "   Epoch 21 | Batch  0/14 | Loss: 0.4811 | Acc: 0.781\n",
      "   Epoch 21 | Batch 10/14 | Loss: 0.4554 | Acc: 0.844\n",
      "   Epoch 21: Train=0.789, Val=0.536, Loss=0.5890, Time=131.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 22 | Batch  0/14 | Loss: 0.4685 | Acc: 0.812\n",
      "   Epoch 22 | Batch 10/14 | Loss: 0.8574 | Acc: 0.688\n",
      "   Epoch 22: Train=0.811, Val=0.554, Loss=0.5436, Time=130.9s\n",
      "--------------------------------------------------\n",
      "   Epoch 23 | Batch  0/14 | Loss: 0.4783 | Acc: 0.812\n",
      "   Epoch 23 | Batch 10/14 | Loss: 0.3643 | Acc: 0.906\n",
      "   Epoch 23: Train=0.838, Val=0.545, Loss=0.4687, Time=132.0s\n",
      "--------------------------------------------------\n",
      "   Epoch 24 | Batch  0/14 | Loss: 0.2082 | Acc: 0.969\n",
      "   Epoch 24 | Batch 10/14 | Loss: 0.3315 | Acc: 0.938\n",
      "   Epoch 24: Train=0.874, Val=0.625, Loss=0.3740, Time=131.1s\n",
      "      ⭐ NEW BEST: 0.625\n",
      "--------------------------------------------------\n",
      "   Epoch 25 | Batch  0/14 | Loss: 0.2622 | Acc: 0.938\n",
      "   Epoch 25 | Batch 10/14 | Loss: 0.3421 | Acc: 0.875\n",
      "   Epoch 25: Train=0.865, Val=0.589, Loss=0.3910, Time=133.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 26 | Batch  0/14 | Loss: 0.4914 | Acc: 0.812\n",
      "   Epoch 26 | Batch 10/14 | Loss: 0.5697 | Acc: 0.812\n",
      "   Epoch 26: Train=0.888, Val=0.571, Loss=0.3483, Time=132.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 27 | Batch  0/14 | Loss: 0.2690 | Acc: 0.875\n",
      "   Epoch 27 | Batch 10/14 | Loss: 0.3040 | Acc: 0.875\n",
      "   Epoch 27: Train=0.899, Val=0.482, Loss=0.2965, Time=133.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 28 | Batch  0/14 | Loss: 0.4341 | Acc: 0.781\n",
      "   Epoch 28 | Batch 10/14 | Loss: 0.3309 | Acc: 0.875\n",
      "   Epoch 28: Train=0.897, Val=0.554, Loss=0.3134, Time=131.4s\n",
      "--------------------------------------------------\n",
      "   Epoch 29 | Batch  0/14 | Loss: 0.3779 | Acc: 0.875\n",
      "   Epoch 29 | Batch 10/14 | Loss: 0.2684 | Acc: 0.875\n",
      "   Epoch 29: Train=0.899, Val=0.607, Loss=0.2711, Time=131.9s\n",
      "--------------------------------------------------\n",
      "   Epoch 30 | Batch  0/14 | Loss: 0.2356 | Acc: 0.906\n",
      "   Epoch 30 | Batch 10/14 | Loss: 0.1140 | Acc: 1.000\n",
      "   Epoch 30: Train=0.930, Val=0.509, Loss=0.2156, Time=133.6s\n",
      "--------------------------------------------------\n",
      "   Epoch 31 | Batch  0/14 | Loss: 0.2328 | Acc: 0.938\n",
      "   Epoch 31 | Batch 10/14 | Loss: 0.1917 | Acc: 0.906\n",
      "   Epoch 31: Train=0.890, Val=0.554, Loss=0.2858, Time=131.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 32 | Batch  0/14 | Loss: 0.0808 | Acc: 1.000\n",
      "   Epoch 32 | Batch 10/14 | Loss: 0.4752 | Acc: 0.875\n",
      "   Epoch 32: Train=0.894, Val=0.634, Loss=0.3053, Time=131.2s\n",
      "      ⭐ NEW BEST: 0.634\n",
      "--------------------------------------------------\n",
      "   Epoch 33 | Batch  0/14 | Loss: 0.3081 | Acc: 0.906\n",
      "   Epoch 33 | Batch 10/14 | Loss: 0.3116 | Acc: 0.906\n",
      "   Epoch 33: Train=0.915, Val=0.464, Loss=0.2525, Time=132.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 34 | Batch  0/14 | Loss: 0.2813 | Acc: 0.906\n",
      "   Epoch 34 | Batch 10/14 | Loss: 0.1433 | Acc: 0.969\n",
      "   Epoch 34: Train=0.928, Val=0.643, Loss=0.2247, Time=134.5s\n",
      "      ⭐ NEW BEST: 0.643\n",
      "--------------------------------------------------\n",
      "   Epoch 35 | Batch  0/14 | Loss: 0.3424 | Acc: 0.906\n",
      "   Epoch 35 | Batch 10/14 | Loss: 0.1398 | Acc: 0.938\n",
      "   Epoch 35: Train=0.919, Val=0.554, Loss=0.2479, Time=136.6s\n",
      "--------------------------------------------------\n",
      "   Epoch 36 | Batch  0/14 | Loss: 0.3594 | Acc: 0.875\n",
      "   Epoch 36 | Batch 10/14 | Loss: 0.1974 | Acc: 0.906\n",
      "   Epoch 36: Train=0.928, Val=0.580, Loss=0.2176, Time=132.7s\n",
      "--------------------------------------------------\n",
      "   Epoch 37 | Batch  0/14 | Loss: 0.1288 | Acc: 0.938\n",
      "   Epoch 37 | Batch 10/14 | Loss: 0.3485 | Acc: 0.906\n",
      "   Epoch 37: Train=0.917, Val=0.536, Loss=0.2210, Time=130.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 38 | Batch  0/14 | Loss: 0.1869 | Acc: 0.875\n",
      "   Epoch 38 | Batch 10/14 | Loss: 0.2637 | Acc: 0.875\n",
      "   Epoch 38: Train=0.921, Val=0.652, Loss=0.2211, Time=130.9s\n",
      "      ⭐ NEW BEST: 0.652\n",
      "--------------------------------------------------\n",
      "   Epoch 39 | Batch  0/14 | Loss: 0.1909 | Acc: 0.969\n",
      "   Epoch 39 | Batch 10/14 | Loss: 0.3682 | Acc: 0.906\n",
      "   Epoch 39: Train=0.930, Val=0.616, Loss=0.1966, Time=130.6s\n",
      "--------------------------------------------------\n",
      "   Epoch 40 | Batch  0/14 | Loss: 0.0472 | Acc: 1.000\n",
      "   Epoch 40 | Batch 10/14 | Loss: 0.3655 | Acc: 0.906\n",
      "   Epoch 40: Train=0.937, Val=0.652, Loss=0.2080, Time=131.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 41 | Batch  0/14 | Loss: 0.1368 | Acc: 0.969\n",
      "   Epoch 41 | Batch 10/14 | Loss: 0.0845 | Acc: 1.000\n",
      "   Epoch 41: Train=0.892, Val=0.554, Loss=0.3393, Time=135.0s\n",
      "--------------------------------------------------\n",
      "   Epoch 42 | Batch  0/14 | Loss: 0.1677 | Acc: 0.938\n",
      "   Epoch 42 | Batch 10/14 | Loss: 0.2915 | Acc: 0.812\n",
      "   Epoch 42: Train=0.883, Val=0.643, Loss=0.3008, Time=130.3s\n",
      "--------------------------------------------------\n",
      "   Epoch 43 | Batch  0/14 | Loss: 0.1944 | Acc: 0.906\n",
      "   Epoch 43 | Batch 10/14 | Loss: 0.0963 | Acc: 0.969\n",
      "   Epoch 43: Train=0.917, Val=0.562, Loss=0.2271, Time=134.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 44 | Batch  0/14 | Loss: 0.1140 | Acc: 0.969\n",
      "   Epoch 44 | Batch 10/14 | Loss: 0.0491 | Acc: 1.000\n",
      "   Epoch 44: Train=0.944, Val=0.527, Loss=0.1765, Time=129.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 45 | Batch  0/14 | Loss: 0.2060 | Acc: 0.938\n",
      "   Epoch 45 | Batch 10/14 | Loss: 0.3469 | Acc: 0.875\n",
      "   Epoch 45: Train=0.930, Val=0.554, Loss=0.2140, Time=130.3s\n",
      "--------------------------------------------------\n",
      "   Epoch 46 | Batch  0/14 | Loss: 0.0908 | Acc: 0.969\n",
      "   Epoch 46 | Batch 10/14 | Loss: 0.0871 | Acc: 1.000\n",
      "   Epoch 46: Train=0.939, Val=0.571, Loss=0.2163, Time=130.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 47 | Batch  0/14 | Loss: 0.0488 | Acc: 1.000\n",
      "   Epoch 47 | Batch 10/14 | Loss: 0.1219 | Acc: 0.938\n",
      "   Epoch 47: Train=0.957, Val=0.598, Loss=0.1243, Time=130.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 48 | Batch  0/14 | Loss: 0.1171 | Acc: 0.969\n",
      "   Epoch 48 | Batch 10/14 | Loss: 0.0845 | Acc: 0.969\n",
      "   Epoch 48: Train=0.921, Val=0.625, Loss=0.2279, Time=130.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 49 | Batch  0/14 | Loss: 0.0391 | Acc: 1.000\n",
      "   Epoch 49 | Batch 10/14 | Loss: 0.1061 | Acc: 0.906\n",
      "   Epoch 49: Train=0.942, Val=0.607, Loss=0.1909, Time=131.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 50 | Batch  0/14 | Loss: 0.1747 | Acc: 0.938\n",
      "   Epoch 50 | Batch 10/14 | Loss: 0.2411 | Acc: 0.938\n",
      "   Epoch 50: Train=0.933, Val=0.634, Loss=0.1979, Time=133.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 51 | Batch  0/14 | Loss: 0.0964 | Acc: 0.969\n",
      "   Epoch 51 | Batch 10/14 | Loss: 0.1326 | Acc: 0.969\n",
      "   Epoch 51: Train=0.966, Val=0.598, Loss=0.1097, Time=132.4s\n",
      "--------------------------------------------------\n",
      "   Epoch 52 | Batch  0/14 | Loss: 0.1335 | Acc: 0.969\n",
      "   Epoch 52 | Batch 10/14 | Loss: 1.0272 | Acc: 0.750\n",
      "   Epoch 52: Train=0.964, Val=0.652, Loss=0.1459, Time=132.6s\n",
      "--------------------------------------------------\n",
      "   Epoch 53 | Batch  0/14 | Loss: 0.0765 | Acc: 1.000\n",
      "   Epoch 53 | Batch 10/14 | Loss: 0.0340 | Acc: 1.000\n",
      "   Epoch 53: Train=0.984, Val=0.661, Loss=0.0631, Time=134.9s\n",
      "      ⭐ NEW BEST: 0.661\n",
      "--------------------------------------------------\n",
      "   Epoch 54 | Batch  0/14 | Loss: 0.0719 | Acc: 0.969\n",
      "   Epoch 54 | Batch 10/14 | Loss: 0.0724 | Acc: 0.969\n",
      "   Epoch 54: Train=0.969, Val=0.643, Loss=0.0937, Time=132.9s\n",
      "--------------------------------------------------\n",
      "   Epoch 55 | Batch  0/14 | Loss: 0.0196 | Acc: 1.000\n",
      "   Epoch 55 | Batch 10/14 | Loss: 0.2955 | Acc: 0.906\n",
      "   Epoch 55: Train=0.962, Val=0.634, Loss=0.1190, Time=137.0s\n",
      "--------------------------------------------------\n",
      "   Epoch 56 | Batch  0/14 | Loss: 0.1460 | Acc: 0.969\n",
      "   Epoch 56 | Batch 10/14 | Loss: 0.2510 | Acc: 0.969\n",
      "   Epoch 56: Train=0.978, Val=0.634, Loss=0.0874, Time=132.5s\n",
      "--------------------------------------------------\n",
      "   Epoch 57 | Batch  0/14 | Loss: 0.0227 | Acc: 1.000\n",
      "   Epoch 57 | Batch 10/14 | Loss: 0.0271 | Acc: 1.000\n",
      "   Epoch 57: Train=0.984, Val=0.652, Loss=0.0566, Time=131.2s\n",
      "--------------------------------------------------\n",
      "   Epoch 58 | Batch  0/14 | Loss: 0.0559 | Acc: 0.969\n",
      "   Epoch 58 | Batch 10/14 | Loss: 0.1179 | Acc: 0.906\n",
      "   Epoch 58: Train=0.964, Val=0.625, Loss=0.0974, Time=134.1s\n",
      "--------------------------------------------------\n",
      "   Epoch 59 | Batch  0/14 | Loss: 0.0440 | Acc: 1.000\n",
      "   Epoch 59 | Batch 10/14 | Loss: 0.1429 | Acc: 0.969\n",
      "   Epoch 59: Train=0.987, Val=0.643, Loss=0.0630, Time=131.0s\n",
      "--------------------------------------------------\n",
      "   Epoch 60 | Batch  0/14 | Loss: 0.0347 | Acc: 1.000\n",
      "   Epoch 60 | Batch 10/14 | Loss: 0.1881 | Acc: 0.969\n",
      "   Epoch 60: Train=0.991, Val=0.643, Loss=0.0633, Time=360.3s\n",
      "--------------------------------------------------\n",
      "   Epoch 61 | Batch  0/14 | Loss: 0.0121 | Acc: 1.000\n",
      "   Epoch 61 | Batch 10/14 | Loss: 0.0336 | Acc: 1.000\n",
      "   Epoch 61: Train=0.984, Val=0.643, Loss=0.0537, Time=130.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 62 | Batch  0/14 | Loss: 0.0857 | Acc: 0.969\n",
      "   Epoch 62 | Batch 10/14 | Loss: 0.0608 | Acc: 0.969\n",
      "   Epoch 62: Train=0.984, Val=0.643, Loss=0.0568, Time=132.3s\n",
      "--------------------------------------------------\n",
      "   Epoch 63 | Batch  0/14 | Loss: 0.0322 | Acc: 1.000\n",
      "   Epoch 63 | Batch 10/14 | Loss: 0.0119 | Acc: 1.000\n",
      "   Epoch 63: Train=0.991, Val=0.616, Loss=0.0479, Time=132.8s\n",
      "--------------------------------------------------\n",
      "   Epoch 64 | Batch  0/14 | Loss: 0.0586 | Acc: 1.000\n",
      "   Epoch 64 | Batch 10/14 | Loss: 0.1311 | Acc: 0.969\n",
      "   Epoch 64: Train=0.978, Val=0.634, Loss=0.0922, Time=134.0s\n",
      "--------------------------------------------------\n",
      "   Epoch 65 | Batch  0/14 | Loss: 0.0203 | Acc: 1.000\n",
      "   Epoch 65 | Batch 10/14 | Loss: 0.3045 | Acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "# PointNet Training Loop\n",
    "def train_pointnet(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience):\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"🚀 POINTNET TRAINING (3D Point Clouds)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for batch_idx, (points, labels) in enumerate(train_loader):\n",
    "            points, labels = points.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(points)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += accuracy_metric(outputs, labels)\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            # Progress logging\n",
    "            if batch_idx % 10 == 0:\n",
    "                batch_acc = accuracy_metric(outputs, labels) / labels.size(0)\n",
    "                print(f\"   Epoch {epoch+1:2d} | Batch {batch_idx:2d}/{len(train_loader)} | Loss: {loss.item():.4f} | Acc: {batch_acc:.3f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for points, labels in val_loader:\n",
    "                points, labels = points.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(points)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += accuracy_metric(outputs, labels)\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"   Epoch {epoch+1:2d}: Train={train_acc:.3f}, Val={val_acc:.3f}, \"\n",
    "              f\"Loss={train_loss:.4f}, Time={epoch_time:.1f}s\")\n",
    "        \n",
    "        # Best model tracking\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"      ⭐ NEW BEST: {best_val_acc:.3f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"   ⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"✅ Best model restored: {best_val_acc:.3f}\")\n",
    "    \n",
    "    return history, best_val_acc\n",
    "\n",
    "# Start training\n",
    "print(\"🌳 Starting PointNet Training...\")\n",
    "start_time = time.time()\n",
    "history, best_val_acc = train_pointnet(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "    NUM_EPOCHS, PATIENCE\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n⏱️ Training completed in {training_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PointNet Evaluation and Results\n",
    "print(\"📊 POINTNET EVALUATION (3D Point Clouds)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test evaluation\n",
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for points, labels in test_loader:\n",
    "        points, labels = points.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(points)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        test_correct += torch.sum(preds == labels).item()\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print(f\"🌳 PointNet Results:\")\n",
    "print(f\"   Training time: {training_time:.1f}s\")\n",
    "print(f\"   Best val accuracy: {best_val_acc:.3f}\")\n",
    "print(f\"   Test accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"   Method: Direct 3D Point Cloud Processing\")\n",
    "print(f\"   Status: {'✅ SUCCESS' if test_accuracy > 0.7 else '⚠️ ROOM FOR IMPROVEMENT'}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n📋 Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training curves\n",
    "plt.subplot(1, 3, 1)\n",
    "epochs = range(1, len(history['train_acc']) + 1)\n",
    "plt.plot(epochs, history['train_acc'], 'b-', label='Train Acc')\n",
    "plt.plot(epochs, history['val_acc'], 'r-', label='Val Acc')\n",
    "plt.axhline(y=test_accuracy, color='g', linestyle='--', label=f'Test Acc ({test_accuracy:.3f})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('PointNet Training Progress\\n(3D Point Clouds)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss')\n",
    "plt.plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'PointNet Results\\n{test_accuracy:.1%} accuracy')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#evaluation metrics\n",
    "\n",
    "print(f\"Overall Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"Balanced Accuracy: {accuracy_score(all_labels, all_preds):.3f}\")\n",
    "print(f\"F1 Score: {f1_score(all_labels, all_preds, average='weighted'):.3f}\")\n",
    "precision = precision_score(all_labels, all_preds, average=None)\n",
    "for i, p in enumerate(precision):\n",
    "    print(f\"   Precision for class {class_names[i]}: {p:.3f}\")\n",
    "print(f\"Execution Time: {training_time:.3f}s\")\n",
    "\n",
    "\n",
    "#• Number of Parameters (Model Complexity)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of Parameters: {num_params:,}\")\n",
    "\n",
    "\n",
    "# Save model\n",
    "model_path = \"../../../models/pointnet_3d_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_names': class_names,\n",
    "    'label_encoder': label_encoder,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'history': history,\n",
    "    'model_config': {\n",
    "        'num_classes': num_classes,\n",
    "        'num_points': NUM_POINTS\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"\\n💾 PointNet model saved: {model_path}\")\n",
    "print(f\"🌳 POINTNET 3D CLASSIFICATION COMPLETE!\")\n",
    "print(f\"   Method: Direct 3D point cloud processing\")\n",
    "print(f\"   Features: FPS sampling + normalization + PointNet\")\n",
    "print(f\"   Final accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"   3D deep learning implementation ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
